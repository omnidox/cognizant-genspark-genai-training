{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install required libraries\n", "!pip install torch tensorflow transformers datasets scikit-learn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check GPU availability\n", "import torch\n", "print(\"GPU Available:\", torch.cuda.is_available())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load and preprocess the IMDB dataset\n", "from datasets import load_dataset\n", "from transformers import AutoTokenizer\n", "\n", "dataset = load_dataset(\"imdb\")\n", "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n", "\n", "def preprocess_function(examples):\n", "    return tokenizer(examples['text'], truncation=True, padding=True)\n", "\n", "tokenized_dataset = dataset.map(preprocess_function, batched=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load Pretrained DistilBERT Model\n", "from transformers import AutoModelForSequenceClassification\n", "\n", "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define Training Arguments and Trainer\n", "from transformers import Trainer, TrainingArguments\n", "\n", "training_args = TrainingArguments(\n", "    output_dir=\"./results\",\n", "    evaluation_strategy=\"epoch\",\n", "    learning_rate=2e-5,\n", "    per_device_train_batch_size=16,\n", "    per_device_eval_batch_size=16,\n", "    num_train_epochs=3,\n", "    weight_decay=0.01,\n", "    logging_dir=\"./logs\",\n", "    logging_steps=200,\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_dataset[\"train\"],\n", "    eval_dataset=tokenized_dataset[\"test\"],\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fine-Tune the Model\n", "trainer.train()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save the Fine-Tuned Model\n", "model.save_pretrained(\"./fine_tuned_model\")\n", "tokenizer.save_pretrained(\"./fine_tuned_model\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate the Model\n", "results = trainer.evaluate()\n", "print(\"Evaluation Results:\", results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Detailed Metrics Evaluation\n", "from sklearn.metrics import classification_report\n", "import numpy as np\n", "\n", "predictions = trainer.predict(tokenized_dataset[\"test\"])\n", "y_pred = np.argmax(predictions.predictions, axis=1)\n", "y_true = tokenized_dataset[\"test\"][\"label\"]\n", "\n", "print(classification_report(y_true, y_pred))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Reflection & Suggestions for Improvement\n", "if results['eval_accuracy'] < 0.90:\n", "    print(\"\\nSuggested Improvements:\")\n", "    print(\"- Increase the number of training epochs.\")\n", "    print(\"- Experiment with different learning rates.\")\n", "    print(\"- Use data augmentation techniques to improve generalization.\")\n", "    print(\"- Fine-tune on a larger dataset for better accuracy.\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}