{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b3bf14",
   "metadata": {},
   "source": [
    "# **Fine-Tuning DistilBERT for Multi-Label Classification**\n",
    "### Kaggle Multi-Label Classification Dataset\n",
    "#### Research Paper Title Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf15417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch transformers datasets scikit-learn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from Kaggle\n",
    "dataset_path = \"/kaggle/input/multilabel-classification-dataset/train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d42850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display column names\n",
    "print(df.columns)\n",
    "\n",
    "# Check label distribution\n",
    "df.iloc[:, 1:].sum().plot(kind=\"bar\", title=\"Category Distribution\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take a smaller subset for faster training\n",
    "df_subset = df.sample(n=2000, random_state=42).reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2636c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load DistilBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Convert dataset to Hugging Face format\n",
    "dataset = Dataset.from_pandas(df_subset)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38390278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained DistilBERT model\n",
    "num_labels = 6  # Number of categories\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c806b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02292aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define compute metrics function\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    labels = labels.numpy()\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13278711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa168b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50849e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = trainer.predict(tokenized_dataset)\n",
    "y_pred = (torch.sigmoid(torch.tensor(predictions.predictions)) > 0.5).int().numpy()\n",
    "y_true = tokenized_dataset[\"label\"]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a94bc",
   "metadata": {},
   "source": [
    "## **Analysis & Future Improvements**\n",
    "**Findings:**\n",
    "- DistilBERT successfully fine-tuned for multi-label classification.\n",
    "- Binary Cross-Entropy loss used.\n",
    "- Micro-averaged F1-score was the main evaluation metric.\n",
    "\n",
    "**Future Improvements:**\n",
    "- Fine-tune on a larger dataset.\n",
    "- Experiment with different architectures like `bert-base-uncased`.\n",
    "- Implement cross-validation for improved generalization.\n",
    "\n",
    "ðŸš€ **Next Steps:** Test on new research paper titles and evaluate real-world performance!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
