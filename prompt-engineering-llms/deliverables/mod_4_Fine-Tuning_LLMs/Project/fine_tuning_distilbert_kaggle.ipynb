{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8e19c8",
   "metadata": {},
   "source": [
    "# **Fine-Tuning DistilBERT for Multi-Label Classification**\n",
    "### Kaggle Multi-Label Classification Dataset\n",
    "#### Research Paper Title Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d682961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch transformers datasets scikit-learn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6aae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset from Kaggle\n",
    "dataset_path = \"/kaggle/input/multilabel-classification-dataset/train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Rename columns to match what the notebook expects\n",
    "df.rename(columns={\"TITLE\": \"title\", \"ABSTRACT\": \"abstract\"}, inplace=True)\n",
    "\n",
    "# Combine title and abstract into a single text column\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"abstract\"]\n",
    "\n",
    "# Define label columns\n",
    "label_columns = [\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]\n",
    "\n",
    "# Ensure labels are binary (0 or 1)\n",
    "df[label_columns] = df[label_columns].astype(int)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_subset = df[[\"text\"] + label_columns]\n",
    "\n",
    "# Display first few rows\n",
    "df_subset.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254865bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display column names\n",
    "print(df_subset.columns)\n",
    "\n",
    "# Check label distribution\n",
    "df_subset[label_columns].sum().plot(kind=\"bar\", title=\"Category Distribution\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take a smaller subset for faster training\n",
    "df_subset = df_subset.sample(n=2000, random_state=42).reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168661b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load DistilBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Convert dataset to Hugging Face format\n",
    "dataset = Dataset.from_pandas(df_subset)\n",
    "\n",
    "# Ensure labels are extracted correctly\n",
    "def preprocess_function(examples):\n",
    "    examples[\"labels\"] = [examples[col] for col in label_columns]\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bcb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained DistilBERT model\n",
    "num_labels = len(label_columns)  # Number of categories\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define compute metrics function for multi-label classification\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    labels = labels.numpy()\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"micro\")\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bf981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = trainer.predict(tokenized_dataset)\n",
    "y_pred = (torch.sigmoid(torch.tensor(predictions.predictions)) > 0.5).int().numpy()\n",
    "y_true = np.array([tokenized_dataset[i][\"labels\"] for i in range(len(tokenized_dataset))])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=label_columns))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ff641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8c033",
   "metadata": {},
   "source": [
    "## **Analysis & Future Improvements**\n",
    "**Findings:**\n",
    "- DistilBERT successfully fine-tuned for multi-label classification.\n",
    "- Binary Cross-Entropy loss used.\n",
    "- Micro-averaged F1-score was the main evaluation metric.\n",
    "\n",
    "**Future Improvements:**\n",
    "- Fine-tune on a larger dataset.\n",
    "- Experiment with different architectures like `bert-base-uncased`.\n",
    "- Implement cross-validation for improved generalization.\n",
    "\n",
    "ðŸš€ **Next Steps:** Test on new research paper titles and evaluate real-world performance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
