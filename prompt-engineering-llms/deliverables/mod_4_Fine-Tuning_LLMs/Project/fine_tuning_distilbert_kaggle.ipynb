{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":""}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d9b3bf14","cell_type":"markdown","source":"# **Fine-Tuning DistilBERT for Multi-Label Classification**\n### Kaggle Multi-Label Classification Dataset\n#### Research Paper Title Categorization","metadata":{}},{"id":"baf15417","cell_type":"code","source":"\n!pip install torch transformers datasets scikit-learn\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"c34a3cb8","cell_type":"code","source":"\nimport pandas as pd\n\n# Load dataset from Kaggle\ndataset_path = \"/kaggle/input/multilabel-classification-dataset/train.csv\"\ndf = pd.read_csv(dataset_path)\n\n# Display first few rows\ndf.head()\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"96d42850","cell_type":"code","source":"\n# Display column names\nprint(df.columns)\n\n# Check label distribution\ndf.iloc[:, 1:].sum().plot(kind=\"bar\", title=\"Category Distribution\")\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"1185302c","cell_type":"code","source":"\n# Take a smaller subset for faster training\ndf_subset = df.sample(n=2000, random_state=42).reset_index(drop=True)\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"d2636c86","cell_type":"code","source":"\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\n# Load DistilBERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Tokenize function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n\n# Convert dataset to Hugging Face format\ndataset = Dataset.from_pandas(df_subset)\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"38390278","cell_type":"code","source":"\nimport torch\nfrom transformers import AutoModelForSequenceClassification\n\n# Load pre-trained DistilBERT model\nnum_labels = 6  # Number of categories\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=num_labels,\n    problem_type=\"multi_label_classification\"\n)\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"3c806b77","cell_type":"code","source":"\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01\n)\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"b02292aa","cell_type":"code","source":"\nfrom transformers import Trainer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Define compute metrics function\ndef compute_metrics(pred):\n    logits, labels = pred\n    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n    labels = labels.numpy()\n    accuracy = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average=\"micro\")\n    return {\"accuracy\": accuracy, \"f1\": f1}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_dataset,\n    compute_metrics=compute_metrics\n)\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"13278711","cell_type":"code","source":"\ntrainer.train()\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"aa168b66","cell_type":"code","source":"\nresults = trainer.evaluate()\nprint(results)\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"50849e2d","cell_type":"code","source":"\npredictions = trainer.predict(tokenized_dataset)\ny_pred = (torch.sigmoid(torch.tensor(predictions.predictions)) > 0.5).int().numpy()\ny_true = tokenized_dataset[\"label\"]\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred))\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"a8f3e3d1","cell_type":"code","source":"\nmodel.save_pretrained(\"./fine_tuned_model\")\ntokenizer.save_pretrained(\"./fine_tuned_model\")\n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"578a94bc","cell_type":"markdown","source":"## **Analysis & Future Improvements**\n**Findings:**\n- DistilBERT successfully fine-tuned for multi-label classification.\n- Binary Cross-Entropy loss used.\n- Micro-averaged F1-score was the main evaluation metric.\n\n**Future Improvements:**\n- Fine-tune on a larger dataset.\n- Experiment with different architectures like `bert-base-uncased`.\n- Implement cross-validation for improved generalization.\n\nðŸš€ **Next Steps:** Test on new research paper titles and evaluate real-world performance!","metadata":{}}]}