{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d8f76ba",
   "metadata": {},
   "source": [
    "# Google Colab Setup for LLM Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280370d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Install Required Libraries\n",
    "!pip install transformers openai sentence-transformers scikit-learn matplotlib streamlit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Import Required Libraries\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911fef2",
   "metadata": {},
   "source": [
    "## Step 3: Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your OpenAI API Key (Replace 'YOUR_OPENAI_API_KEY' with your actual key)\n",
    "openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c4d22",
   "metadata": {},
   "source": [
    "## Project 1: Exploring Tokenization and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209edd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_text(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    return tokenizer, tokens\n",
    "\n",
    "def visualize_embeddings(text):\n",
    "    tokenizer, tokens = tokenize_text(text)\n",
    "    model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.squeeze().numpy()\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])\n",
    "    for i, token in enumerate(tokens):\n",
    "        plt.annotate(token, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "    plt.title(\"Token Embeddings Visualization (PCA)\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58e853",
   "metadata": {},
   "source": [
    "## Project 2: Crafting the Perfect Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f08b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gpt_response(prompt, text):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=f\"{prompt}\\n\\n{text}\",\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "generic_prompt = \"Summarize the following article.\"\n",
    "detailed_prompt = \"Summarize the following article, focusing on the main arguments and conclusions.\"\n",
    "specific_prompt = \"In 100 words or less, summarize the key findings of the following research article, emphasizing its implications for future studies.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ae9b7",
   "metadata": {},
   "source": [
    "## Project 3: Building a Mini Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_streamlit_app():\n",
    "    st.title('LLM-Powered Application')\n",
    "    user_input = st.text_area('Enter your text here:')\n",
    "    if st.button('Process'):\n",
    "        response = get_gpt_response(specific_prompt, user_input)\n",
    "        st.write('Response:', response)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089a446",
   "metadata": {},
   "source": [
    "## Project 4: Advanced Prompt Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e52045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain_prompt = \"To solve the problem, we need to follow these steps:\"\n",
    "few_shot_prompt = \"\"\"Example 1:\n",
    "Input: [input text]\n",
    "Output: [desired output]\n",
    "\n",
    "Example 2:\n",
    "Input: [input text]\n",
    "Output: [desired output]\n",
    "\n",
    "Now, process the following input accordingly.\"\"\"\n",
    "role_play_prompt = \"As a historian, explain the significance of the Renaissance.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8939b7",
   "metadata": {},
   "source": [
    "## Running the Streamlit App in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdef274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment the below lines to run a specific project\n",
    "# visualize_embeddings(\"Your sample text here.\")\n",
    "# run_streamlit_app()\n",
    "\n",
    "# To run Streamlit in Colab:\n",
    "# !streamlit run your_script.py & npx localtunnel --port 8501\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}