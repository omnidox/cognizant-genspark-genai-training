{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":31436,"sourceType":"datasetVersion","datasetId":19447}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìä **New York Times Comments Dataset Analysis**\nThis notebook analyzes the New York Times Comments dataset available on Kaggle.\nWe will extract metadata, check for missing values, and summarize the structure of the dataset before proceeding with text analysis.","metadata":{}},{"cell_type":"markdown","source":"## **üìå Step 1: Setup the Environment**\nWe start by importing the necessary libraries and listing all available files in the dataset.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Set path to dataset (Kaggle users should adjust as needed)\ndataset_path = \"/kaggle/input/nyt-comments\"\n\n# List all files in the dataset\nfiles = os.listdir(dataset_path)\nprint(\"Files in dataset:\\n\", files)","metadata":{"execution":{"iopub.execute_input":"2025-02-28T16:56:51.639461Z","iopub.status.busy":"2025-02-28T16:56:51.639288Z","iopub.status.idle":"2025-02-28T16:56:52.686453Z","shell.execute_reply":"2025-02-28T16:56:52.685451Z","shell.execute_reply.started":"2025-02-28T16:56:51.639442Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files in dataset:\n"," ['CommentsFeb2018.csv', 'ArticlesFeb2017.csv', 'CommentsApril2018.csv', 'ArticlesJan2017.csv', 'ArticlesMay2017.csv', 'CommentsJan2017.csv', 'CommentsMarch2017.csv', 'CommentsMay2017.csv', 'CommentsMarch2018.csv', 'CommentsApril2017.csv', 'ArticlesMarch2017.csv', 'ArticlesApril2017.csv', 'CommentsFeb2017.csv', 'ArticlesJan2018.csv', 'ArticlesFeb2018.csv', 'ArticlesMarch2018.csv', 'CommentsJan2018.csv', 'ArticlesApril2018.csv']\n"]}],"execution_count":1},{"cell_type":"markdown","source":"## **üìå Step 2: Load & Inspect Data**\nLet's load files (e.g., `ArticlesJan2017.csv` and `CommentsApril2017.csv`) to inspect its structure.","metadata":{}},{"cell_type":"code","source":"# Load an example file to inspect its structure\nsample_file = \"ArticlesJan2017.csv\"  # You can change this to any file in the dataset\ndf = pd.read_csv(os.path.join(dataset_path, sample_file))\n\n# Display first few rows\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2025-02-28T16:57:18.603779Z","iopub.status.busy":"2025-02-28T16:57:18.603437Z","iopub.status.idle":"2025-02-28T16:57:18.656793Z","shell.execute_reply":"2025-02-28T16:57:18.655880Z","shell.execute_reply.started":"2025-02-28T16:57:18.603745Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>articleID</th>\n","      <th>abstract</th>\n","      <th>byline</th>\n","      <th>documentType</th>\n","      <th>headline</th>\n","      <th>keywords</th>\n","      <th>multimedia</th>\n","      <th>newDesk</th>\n","      <th>printPage</th>\n","      <th>pubDate</th>\n","      <th>sectionName</th>\n","      <th>snippet</th>\n","      <th>source</th>\n","      <th>typeOfMaterial</th>\n","      <th>webURL</th>\n","      <th>articleWordCount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>58691a5795d0e039260788b9</td>\n","      <td>NaN</td>\n","      <td>By JENNIFER STEINHAUER</td>\n","      <td>article</td>\n","      <td>G.O.P. Leadership Poised to Topple Obama‚Äôs Pi...</td>\n","      <td>['United States Politics and Government', 'Law...</td>\n","      <td>1</td>\n","      <td>National</td>\n","      <td>1</td>\n","      <td>2017-01-01 15:03:38</td>\n","      <td>Politics</td>\n","      <td>The most powerful and ambitious Republican-led...</td>\n","      <td>The New York Times</td>\n","      <td>News</td>\n","      <td>https://www.nytimes.com/2017/01/01/us/politics...</td>\n","      <td>1324</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>586967bf95d0e03926078915</td>\n","      <td>NaN</td>\n","      <td>By MARK LANDLER</td>\n","      <td>article</td>\n","      <td>Fractured World Tested the Hope of a Young Pre...</td>\n","      <td>['Obama, Barack', 'Afghanistan', 'United State...</td>\n","      <td>1</td>\n","      <td>Foreign</td>\n","      <td>1</td>\n","      <td>2017-01-01 20:34:00</td>\n","      <td>Asia Pacific</td>\n","      <td>A strategy that went from a ‚Äúgood war‚Äù to the ...</td>\n","      <td>The New York Times</td>\n","      <td>News</td>\n","      <td>https://www.nytimes.com/2017/01/01/world/asia/...</td>\n","      <td>2836</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>58698a1095d0e0392607894a</td>\n","      <td>NaN</td>\n","      <td>By CAITLIN LOVINGER</td>\n","      <td>article</td>\n","      <td>Little Troublemakers</td>\n","      <td>['Crossword Puzzles', 'Boxing Day', 'Holidays ...</td>\n","      <td>1</td>\n","      <td>Games</td>\n","      <td>0</td>\n","      <td>2017-01-01 23:00:24</td>\n","      <td>Unknown</td>\n","      <td>Chuck Deodene puts us in a bubbly mood.</td>\n","      <td>The New York Times</td>\n","      <td>News</td>\n","      <td>https://www.nytimes.com/2017/01/01/crosswords/...</td>\n","      <td>445</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5869911a95d0e0392607894e</td>\n","      <td>NaN</td>\n","      <td>By JOCHEN BITTNER</td>\n","      <td>article</td>\n","      <td>Angela Merkel, Russia‚Äôs Next Target</td>\n","      <td>['Cyberwarfare and Defense', 'Presidential Ele...</td>\n","      <td>1</td>\n","      <td>OpEd</td>\n","      <td>15</td>\n","      <td>2017-01-01 23:30:27</td>\n","      <td>Unknown</td>\n","      <td>With a friend entering the White House, Vladim...</td>\n","      <td>The New York Times</td>\n","      <td>Op-Ed</td>\n","      <td>https://www.nytimes.com/2017/01/01/opinion/ang...</td>\n","      <td>864</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5869a61795d0e03926078962</td>\n","      <td>NaN</td>\n","      <td>By JIAYIN SHEN</td>\n","      <td>article</td>\n","      <td>Boots for a Stranger on a Bus</td>\n","      <td>['Shoes and Boots', 'Buses', 'New York City']</td>\n","      <td>0</td>\n","      <td>Metro</td>\n","      <td>12</td>\n","      <td>2017-01-02 01:00:02</td>\n","      <td>Unknown</td>\n","      <td>Witnessing an act of generosity on a rainy day.</td>\n","      <td>The New York Times</td>\n","      <td>Brief</td>\n","      <td>https://www.nytimes.com/2017/01/01/nyregion/me...</td>\n","      <td>309</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  articleID abstract                  byline documentType  \\\n","0  58691a5795d0e039260788b9      NaN  By JENNIFER STEINHAUER      article   \n","1  586967bf95d0e03926078915      NaN         By MARK LANDLER      article   \n","2  58698a1095d0e0392607894a      NaN     By CAITLIN LOVINGER      article   \n","3  5869911a95d0e0392607894e      NaN       By JOCHEN BITTNER      article   \n","4  5869a61795d0e03926078962      NaN          By JIAYIN SHEN      article   \n","\n","                                            headline  \\\n","0   G.O.P. Leadership Poised to Topple Obama‚Äôs Pi...   \n","1  Fractured World Tested the Hope of a Young Pre...   \n","2                               Little Troublemakers   \n","3                Angela Merkel, Russia‚Äôs Next Target   \n","4                      Boots for a Stranger on a Bus   \n","\n","                                            keywords  multimedia   newDesk  \\\n","0  ['United States Politics and Government', 'Law...           1  National   \n","1  ['Obama, Barack', 'Afghanistan', 'United State...           1   Foreign   \n","2  ['Crossword Puzzles', 'Boxing Day', 'Holidays ...           1     Games   \n","3  ['Cyberwarfare and Defense', 'Presidential Ele...           1      OpEd   \n","4      ['Shoes and Boots', 'Buses', 'New York City']           0     Metro   \n","\n","   printPage              pubDate   sectionName  \\\n","0          1  2017-01-01 15:03:38      Politics   \n","1          1  2017-01-01 20:34:00  Asia Pacific   \n","2          0  2017-01-01 23:00:24       Unknown   \n","3         15  2017-01-01 23:30:27       Unknown   \n","4         12  2017-01-02 01:00:02       Unknown   \n","\n","                                             snippet              source  \\\n","0  The most powerful and ambitious Republican-led...  The New York Times   \n","1  A strategy that went from a ‚Äúgood war‚Äù to the ...  The New York Times   \n","2            Chuck Deodene puts us in a bubbly mood.  The New York Times   \n","3  With a friend entering the White House, Vladim...  The New York Times   \n","4    Witnessing an act of generosity on a rainy day.  The New York Times   \n","\n","  typeOfMaterial                                             webURL  \\\n","0           News  https://www.nytimes.com/2017/01/01/us/politics...   \n","1           News  https://www.nytimes.com/2017/01/01/world/asia/...   \n","2           News  https://www.nytimes.com/2017/01/01/crosswords/...   \n","3          Op-Ed  https://www.nytimes.com/2017/01/01/opinion/ang...   \n","4          Brief  https://www.nytimes.com/2017/01/01/nyregion/me...   \n","\n","   articleWordCount  \n","0              1324  \n","1              2836  \n","2               445  \n","3               864  \n","4               309  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"execution_count":2},{"cell_type":"code","source":"# Load an example file to inspect its structure\nsample_file_2 = \"CommentsApril2017.csv\"  # You can change this to any file in the dataset\ndf_2 = pd.read_csv(os.path.join(dataset_path, sample_file_2))\n\n# Display first few rows\ndf_2.head()","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:16:25.325466Z","iopub.status.busy":"2025-02-28T17:16:25.325167Z","iopub.status.idle":"2025-02-28T17:16:27.326585Z","shell.execute_reply":"2025-02-28T17:16:27.325724Z","shell.execute_reply.started":"2025-02-28T17:16:25.325441Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-9-5a2998bf1dac>:3: DtypeWarning: Columns (25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df_2 = pd.read_csv(os.path.join(dataset_path, sample_file_2))\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>approveDate</th>\n","      <th>commentBody</th>\n","      <th>commentID</th>\n","      <th>commentSequence</th>\n","      <th>commentTitle</th>\n","      <th>commentType</th>\n","      <th>createDate</th>\n","      <th>depth</th>\n","      <th>editorsSelection</th>\n","      <th>parentID</th>\n","      <th>...</th>\n","      <th>userLocation</th>\n","      <th>userTitle</th>\n","      <th>userURL</th>\n","      <th>inReplyTo</th>\n","      <th>articleID</th>\n","      <th>sectionName</th>\n","      <th>newDesk</th>\n","      <th>articleWordCount</th>\n","      <th>printPage</th>\n","      <th>typeOfMaterial</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1491245186</td>\n","      <td>This project makes me happy to be a 30+ year T...</td>\n","      <td>22022598.0</td>\n","      <td>22022598</td>\n","      <td>&lt;br/&gt;</td>\n","      <td>comment</td>\n","      <td>1.491237e+09</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>Riverside, CA</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>58def1347c459f24986d7c80</td>\n","      <td>Unknown</td>\n","      <td>Insider</td>\n","      <td>716.0</td>\n","      <td>2</td>\n","      <td>News</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1491188619</td>\n","      <td>Stunning photos and reportage. Infuriating tha...</td>\n","      <td>22017350.0</td>\n","      <td>22017350</td>\n","      <td>NaN</td>\n","      <td>comment</td>\n","      <td>1.491180e+09</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>&lt;br/&gt;</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>58def1347c459f24986d7c80</td>\n","      <td>Unknown</td>\n","      <td>Insider</td>\n","      <td>716.0</td>\n","      <td>2</td>\n","      <td>News</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1491188617</td>\n","      <td>Brilliant work from conception to execution. I...</td>\n","      <td>22017334.0</td>\n","      <td>22017334</td>\n","      <td>&lt;br/&gt;</td>\n","      <td>comment</td>\n","      <td>1.491179e+09</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>Raleigh NC</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>58def1347c459f24986d7c80</td>\n","      <td>Unknown</td>\n","      <td>Insider</td>\n","      <td>716.0</td>\n","      <td>2</td>\n","      <td>News</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1491167820</td>\n","      <td>NYT reporters should provide a contributor's l...</td>\n","      <td>22015913.0</td>\n","      <td>22015913</td>\n","      <td>&lt;br/&gt;</td>\n","      <td>comment</td>\n","      <td>1.491150e+09</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>Missouri, USA</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>58def1347c459f24986d7c80</td>\n","      <td>Unknown</td>\n","      <td>Insider</td>\n","      <td>716.0</td>\n","      <td>2</td>\n","      <td>News</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1491167815</td>\n","      <td>Could only have been done in print. Stunning.</td>\n","      <td>22015466.0</td>\n","      <td>22015466</td>\n","      <td>&lt;br/&gt;</td>\n","      <td>comment</td>\n","      <td>1.491147e+09</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>Tucson, Arizona</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>58def1347c459f24986d7c80</td>\n","      <td>Unknown</td>\n","      <td>Insider</td>\n","      <td>716.0</td>\n","      <td>2</td>\n","      <td>News</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 34 columns</p>\n","</div>"],"text/plain":["   approveDate                                        commentBody   commentID  \\\n","0   1491245186  This project makes me happy to be a 30+ year T...  22022598.0   \n","1   1491188619  Stunning photos and reportage. Infuriating tha...  22017350.0   \n","2   1491188617  Brilliant work from conception to execution. I...  22017334.0   \n","3   1491167820  NYT reporters should provide a contributor's l...  22015913.0   \n","4   1491167815     Could only have been done in print. Stunning.   22015466.0   \n","\n","   commentSequence commentTitle commentType    createDate  depth  \\\n","0         22022598        <br/>     comment  1.491237e+09      1   \n","1         22017350          NaN     comment  1.491180e+09      1   \n","2         22017334        <br/>     comment  1.491179e+09      1   \n","3         22015913        <br/>     comment  1.491150e+09      1   \n","4         22015466        <br/>     comment  1.491147e+09      1   \n","\n","   editorsSelection  parentID  ...     userLocation userTitle userURL  \\\n","0             False       0.0  ...    Riverside, CA       NaN     NaN   \n","1             False       0.0  ...            <br/>       NaN     NaN   \n","2             False       0.0  ...       Raleigh NC       NaN     NaN   \n","3             False       0.0  ...    Missouri, USA       NaN     NaN   \n","4             False       0.0  ...  Tucson, Arizona       NaN     NaN   \n","\n","   inReplyTo                 articleID  sectionName  newDesk  \\\n","0          0  58def1347c459f24986d7c80      Unknown  Insider   \n","1          0  58def1347c459f24986d7c80      Unknown  Insider   \n","2          0  58def1347c459f24986d7c80      Unknown  Insider   \n","3          0  58def1347c459f24986d7c80      Unknown  Insider   \n","4          0  58def1347c459f24986d7c80      Unknown  Insider   \n","\n","   articleWordCount printPage  typeOfMaterial  \n","0             716.0         2            News  \n","1             716.0         2            News  \n","2             716.0         2            News  \n","3             716.0         2            News  \n","4             716.0         2            News  \n","\n","[5 rows x 34 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"execution_count":9},{"cell_type":"markdown","source":"## **üìå Step 3: Extract Metadata**\nNow, we extract key metadata, such as column names, data types, and missing values.","metadata":{}},{"cell_type":"code","source":"# Display dataset information\ndf.info()","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:05:15.974425Z","iopub.status.busy":"2025-02-28T17:05:15.974141Z","iopub.status.idle":"2025-02-28T17:05:15.985018Z","shell.execute_reply":"2025-02-28T17:05:15.983795Z","shell.execute_reply.started":"2025-02-28T17:05:15.974402Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 850 entries, 0 to 849\n","Data columns (total 16 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   articleID         850 non-null    object\n"," 1   abstract          40 non-null     object\n"," 2   byline            850 non-null    object\n"," 3   documentType      850 non-null    object\n"," 4   headline          850 non-null    object\n"," 5   keywords          850 non-null    object\n"," 6   multimedia        850 non-null    int64 \n"," 7   newDesk           850 non-null    object\n"," 8   printPage         850 non-null    int64 \n"," 9   pubDate           850 non-null    object\n"," 10  sectionName       850 non-null    object\n"," 11  snippet           850 non-null    object\n"," 12  source            850 non-null    object\n"," 13  typeOfMaterial    850 non-null    object\n"," 14  webURL            850 non-null    object\n"," 15  articleWordCount  850 non-null    int64 \n","dtypes: int64(3), object(13)\n","memory usage: 106.4+ KB\n"]}],"execution_count":4},{"cell_type":"code","source":"# Display dataset information\ndf_2.info()","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:17:03.887136Z","iopub.status.busy":"2025-02-28T17:17:03.886811Z","iopub.status.idle":"2025-02-28T17:17:04.038471Z","shell.execute_reply":"2025-02-28T17:17:04.037633Z","shell.execute_reply.started":"2025-02-28T17:17:03.887108Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 243832 entries, 0 to 243831\n","Data columns (total 34 columns):\n"," #   Column                 Non-Null Count   Dtype  \n","---  ------                 --------------   -----  \n"," 0   approveDate            243832 non-null  int64  \n"," 1   commentBody            243832 non-null  object \n"," 2   commentID              243832 non-null  float64\n"," 3   commentSequence        243832 non-null  int64  \n"," 4   commentTitle           228498 non-null  object \n"," 5   commentType            243832 non-null  object \n"," 6   createDate             243832 non-null  float64\n"," 7   depth                  243832 non-null  int64  \n"," 8   editorsSelection       243832 non-null  bool   \n"," 9   parentID               243832 non-null  float64\n"," 10  parentUserDisplayName  70526 non-null   object \n"," 11  permID                 243832 non-null  object \n"," 12  picURL                 243832 non-null  object \n"," 13  recommendations        243832 non-null  float64\n"," 14  recommendedFlag        0 non-null       float64\n"," 15  replyCount             243832 non-null  float64\n"," 16  reportAbuseFlag        0 non-null       float64\n"," 17  sharing                243832 non-null  int64  \n"," 18  status                 243832 non-null  object \n"," 19  timespeople            243832 non-null  float64\n"," 20  trusted                243832 non-null  float64\n"," 21  updateDate             243832 non-null  int64  \n"," 22  userDisplayName        243755 non-null  object \n"," 23  userID                 243832 non-null  int64  \n"," 24  userLocation           243770 non-null  object \n"," 25  userTitle              41 non-null      object \n"," 26  userURL                5 non-null       object \n"," 27  inReplyTo              243832 non-null  int64  \n"," 28  articleID              243832 non-null  object \n"," 29  sectionName            243832 non-null  object \n"," 30  newDesk                243832 non-null  object \n"," 31  articleWordCount       243832 non-null  float64\n"," 32  printPage              243832 non-null  int64  \n"," 33  typeOfMaterial         243832 non-null  object \n","dtypes: bool(1), float64(10), int64(8), object(15)\n","memory usage: 61.6+ MB\n"]}],"execution_count":10},{"cell_type":"markdown","source":"## **üìå Step 4: Check for Missing Values**\nChecking for missing values in each column.","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nmissing_values = df.isnull().sum()\nmissing_values[missing_values > 0]","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:05:34.208514Z","iopub.status.busy":"2025-02-28T17:05:34.208202Z","iopub.status.idle":"2025-02-28T17:05:34.215820Z","shell.execute_reply":"2025-02-28T17:05:34.215044Z","shell.execute_reply.started":"2025-02-28T17:05:34.208486Z"},"trusted":true},"outputs":[{"data":{"text/plain":["abstract    810\n","dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"execution_count":5},{"cell_type":"code","source":"# Check for missing values\nmissing_values_2 = df_2.isnull().sum()\nmissing_values_2[missing_values_2 > 0]","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:17:52.933397Z","iopub.status.busy":"2025-02-28T17:17:52.933084Z","iopub.status.idle":"2025-02-28T17:17:53.085095Z","shell.execute_reply":"2025-02-28T17:17:53.083659Z","shell.execute_reply.started":"2025-02-28T17:17:52.933370Z"},"trusted":true},"outputs":[{"data":{"text/plain":["commentTitle              15334\n","parentUserDisplayName    173306\n","recommendedFlag          243832\n","reportAbuseFlag          243832\n","userDisplayName              77\n","userLocation                 62\n","userTitle                243791\n","userURL                  243827\n","dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"execution_count":11},{"cell_type":"markdown","source":"## **üìå Step 5: Summary Statistics**\nGenerate a summary of numeric and categorical columns.","metadata":{}},{"cell_type":"code","source":"# Display summary statistics\ndf.describe(include=\"all\").transpose()","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:05:49.994360Z","iopub.status.busy":"2025-02-28T17:05:49.994043Z","iopub.status.idle":"2025-02-28T17:05:50.038845Z","shell.execute_reply":"2025-02-28T17:05:50.037407Z","shell.execute_reply.started":"2025-02-28T17:05:49.994335Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>unique</th>\n","      <th>top</th>\n","      <th>freq</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>articleID</th>\n","      <td>850</td>\n","      <td>850</td>\n","      <td>58691a5795d0e039260788b9</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>abstract</th>\n","      <td>40</td>\n","      <td>40</td>\n","      <td>After losing three limbs in Afghanistan, a Mar...</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>byline</th>\n","      <td>850</td>\n","      <td>434</td>\n","      <td>By THE EDITORIAL BOARD</td>\n","      <td>32</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>documentType</th>\n","      <td>850</td>\n","      <td>2</td>\n","      <td>article</td>\n","      <td>810</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>headline</th>\n","      <td>850</td>\n","      <td>774</td>\n","      <td>Unknown</td>\n","      <td>73</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>keywords</th>\n","      <td>850</td>\n","      <td>717</td>\n","      <td>[]</td>\n","      <td>73</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>multimedia</th>\n","      <td>850.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.927059</td>\n","      <td>0.260193</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>newDesk</th>\n","      <td>850</td>\n","      <td>28</td>\n","      <td>OpEd</td>\n","      <td>175</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>printPage</th>\n","      <td>850.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>7.077647</td>\n","      <td>10.100022</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>12.0</td>\n","      <td>66.0</td>\n","    </tr>\n","    <tr>\n","      <th>pubDate</th>\n","      <td>850</td>\n","      <td>786</td>\n","      <td>2017-02-02 08:21:23</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>sectionName</th>\n","      <td>850</td>\n","      <td>30</td>\n","      <td>Unknown</td>\n","      <td>598</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>snippet</th>\n","      <td>850</td>\n","      <td>846</td>\n","      <td>Look closely at this image, stripped of its ca...</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>source</th>\n","      <td>850</td>\n","      <td>2</td>\n","      <td>The New York Times</td>\n","      <td>844</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>typeOfMaterial</th>\n","      <td>850</td>\n","      <td>11</td>\n","      <td>News</td>\n","      <td>513</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>webURL</th>\n","      <td>850</td>\n","      <td>850</td>\n","      <td>https://www.nytimes.com/2017/01/01/us/politics...</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>articleWordCount</th>\n","      <td>850.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1062.878824</td>\n","      <td>895.597064</td>\n","      <td>25.0</td>\n","      <td>612.5</td>\n","      <td>923.5</td>\n","      <td>1291.0</td>\n","      <td>8784.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  count unique  \\\n","articleID           850    850   \n","abstract             40     40   \n","byline              850    434   \n","documentType        850      2   \n","headline            850    774   \n","keywords            850    717   \n","multimedia        850.0    NaN   \n","newDesk             850     28   \n","printPage         850.0    NaN   \n","pubDate             850    786   \n","sectionName         850     30   \n","snippet             850    846   \n","source              850      2   \n","typeOfMaterial      850     11   \n","webURL              850    850   \n","articleWordCount  850.0    NaN   \n","\n","                                                                top freq  \\\n","articleID                                  58691a5795d0e039260788b9    1   \n","abstract          After losing three limbs in Afghanistan, a Mar...    1   \n","byline                                       By THE EDITORIAL BOARD   32   \n","documentType                                                article  810   \n","headline                                                    Unknown   73   \n","keywords                                                         []   73   \n","multimedia                                                      NaN  NaN   \n","newDesk                                                        OpEd  175   \n","printPage                                                       NaN  NaN   \n","pubDate                                         2017-02-02 08:21:23    4   \n","sectionName                                                 Unknown  598   \n","snippet           Look closely at this image, stripped of its ca...    3   \n","source                                           The New York Times  844   \n","typeOfMaterial                                                 News  513   \n","webURL            https://www.nytimes.com/2017/01/01/us/politics...    1   \n","articleWordCount                                                NaN  NaN   \n","\n","                         mean         std   min    25%    50%     75%     max  \n","articleID                 NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","abstract                  NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","byline                    NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","documentType              NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","headline                  NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","keywords                  NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","multimedia           0.927059    0.260193   0.0    1.0    1.0     1.0     1.0  \n","newDesk                   NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","printPage            7.077647   10.100022   0.0    0.0    1.0    12.0    66.0  \n","pubDate                   NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","sectionName               NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","snippet                   NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","source                    NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","typeOfMaterial            NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","webURL                    NaN         NaN   NaN    NaN    NaN     NaN     NaN  \n","articleWordCount  1062.878824  895.597064  25.0  612.5  923.5  1291.0  8784.0  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"execution_count":6},{"cell_type":"code","source":"# Display summary statistics\ndf_2.describe(include=\"all\").transpose()","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:19:50.766519Z","iopub.status.busy":"2025-02-28T17:19:50.766210Z","iopub.status.idle":"2025-02-28T17:19:51.650057Z","shell.execute_reply":"2025-02-28T17:19:51.649063Z","shell.execute_reply.started":"2025-02-28T17:19:50.766492Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>unique</th>\n","      <th>top</th>\n","      <th>freq</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>approveDate</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1492504461.72283</td>\n","      <td>1330667.064463</td>\n","      <td>1491008047.0</td>\n","      <td>1491783853.25</td>\n","      <td>1492437034.5</td>\n","      <td>1493119511.0</td>\n","      <td>1524346252.0</td>\n","    </tr>\n","    <tr>\n","      <th>commentBody</th>\n","      <td>243832</td>\n","      <td>243169</td>\n","      <td>Well said.</td>\n","      <td>21</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>commentID</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>22188608.516839</td>\n","      <td>185515.004384</td>\n","      <td>21999548.0</td>\n","      <td>22092910.75</td>\n","      <td>22176681.5</td>\n","      <td>22260467.5</td>\n","      <td>26824246.0</td>\n","    </tr>\n","    <tr>\n","      <th>commentSequence</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>22188608.516839</td>\n","      <td>185515.004384</td>\n","      <td>21999548.0</td>\n","      <td>22092910.75</td>\n","      <td>22176681.5</td>\n","      <td>22260467.5</td>\n","      <td>26824246.0</td>\n","    </tr>\n","    <tr>\n","      <th>commentTitle</th>\n","      <td>228498</td>\n","      <td>1</td>\n","      <td>&lt;br/&gt;</td>\n","      <td>228498</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>commentType</th>\n","      <td>243832</td>\n","      <td>3</td>\n","      <td>comment</td>\n","      <td>173277</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>createDate</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1492495336.431272</td>\n","      <td>1328516.276157</td>\n","      <td>1491006872.0</td>\n","      <td>1491768493.75</td>\n","      <td>1492428038.0</td>\n","      <td>1493086582.25</td>\n","      <td>1524345694.0</td>\n","    </tr>\n","    <tr>\n","      <th>depth</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.289425</td>\n","      <td>0.453641</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>editorsSelection</th>\n","      <td>243832</td>\n","      <td>2</td>\n","      <td>False</td>\n","      <td>238159</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>parentID</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>6416284.403421</td>\n","      <td>10055291.992253</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>22051084.75</td>\n","      <td>26426201.0</td>\n","    </tr>\n","    <tr>\n","      <th>parentUserDisplayName</th>\n","      <td>70526</td>\n","      <td>15712</td>\n","      <td>John</td>\n","      <td>351</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>permID</th>\n","      <td>243832</td>\n","      <td>243832</td>\n","      <td>22022598</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>picURL</th>\n","      <td>243832</td>\n","      <td>4282</td>\n","      <td>https://graphics8.nytimes.com/images/apps/time...</td>\n","      <td>201781</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>recommendations</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>17.625172</td>\n","      <td>93.219419</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>9.0</td>\n","      <td>7938.0</td>\n","    </tr>\n","    <tr>\n","      <th>recommendedFlag</th>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>replyCount</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.405734</td>\n","      <td>1.944378</td>\n","      <td>-115.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>361.0</td>\n","    </tr>\n","    <tr>\n","      <th>reportAbuseFlag</th>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>sharing</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.086371</td>\n","      <td>0.280912</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>status</th>\n","      <td>243832</td>\n","      <td>1</td>\n","      <td>approved</td>\n","      <td>243832</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>timespeople</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.999643</td>\n","      <td>0.018886</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>trusted</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.033576</td>\n","      <td>0.180136</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>updateDate</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1492505796.335194</td>\n","      <td>1330736.099802</td>\n","      <td>1491008947.0</td>\n","      <td>1491787044.0</td>\n","      <td>1492437385.5</td>\n","      <td>1493122001.25</td>\n","      <td>1524346252.0</td>\n","    </tr>\n","    <tr>\n","      <th>userDisplayName</th>\n","      <td>243755</td>\n","      <td>46510</td>\n","      <td>John</td>\n","      <td>1214</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>userID</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>49557503.00269</td>\n","      <td>23381533.206869</td>\n","      <td>1045.0</td>\n","      <td>32195826.0</td>\n","      <td>57198758.0</td>\n","      <td>67699953.75</td>\n","      <td>85678080.0</td>\n","    </tr>\n","    <tr>\n","      <th>userLocation</th>\n","      <td>243770</td>\n","      <td>15890</td>\n","      <td>NYC</td>\n","      <td>7292</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>userTitle</th>\n","      <td>41</td>\n","      <td>9</td>\n","      <td>Your Money columnist</td>\n","      <td>18</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>userURL</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>http://www.nytimes.com/column/the-walking-dead...</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>inReplyTo</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>6416193.792542</td>\n","      <td>10055250.263762</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>22051083.25</td>\n","      <td>26426201.0</td>\n","    </tr>\n","    <tr>\n","      <th>articleID</th>\n","      <td>243832</td>\n","      <td>886</td>\n","      <td>58ebb1437c459f24986d96ed</td>\n","      <td>5181</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>sectionName</th>\n","      <td>243832</td>\n","      <td>31</td>\n","      <td>Unknown</td>\n","      <td>137928</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>newDesk</th>\n","      <td>243832</td>\n","      <td>28</td>\n","      <td>OpEd</td>\n","      <td>82519</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>articleWordCount</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1293.667451</td>\n","      <td>946.593427</td>\n","      <td>57.0</td>\n","      <td>828.0</td>\n","      <td>1124.0</td>\n","      <td>1385.0</td>\n","      <td>7832.0</td>\n","    </tr>\n","    <tr>\n","      <th>printPage</th>\n","      <td>243832.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9.458652</td>\n","      <td>11.138473</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>20.0</td>\n","      <td>74.0</td>\n","    </tr>\n","    <tr>\n","      <th>typeOfMaterial</th>\n","      <td>243832</td>\n","      <td>11</td>\n","      <td>News</td>\n","      <td>130770</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          count  unique  \\\n","approveDate            243832.0     NaN   \n","commentBody              243832  243169   \n","commentID              243832.0     NaN   \n","commentSequence        243832.0     NaN   \n","commentTitle             228498       1   \n","commentType              243832       3   \n","createDate             243832.0     NaN   \n","depth                  243832.0     NaN   \n","editorsSelection         243832       2   \n","parentID               243832.0     NaN   \n","parentUserDisplayName     70526   15712   \n","permID                   243832  243832   \n","picURL                   243832    4282   \n","recommendations        243832.0     NaN   \n","recommendedFlag             0.0     NaN   \n","replyCount             243832.0     NaN   \n","reportAbuseFlag             0.0     NaN   \n","sharing                243832.0     NaN   \n","status                   243832       1   \n","timespeople            243832.0     NaN   \n","trusted                243832.0     NaN   \n","updateDate             243832.0     NaN   \n","userDisplayName          243755   46510   \n","userID                 243832.0     NaN   \n","userLocation             243770   15890   \n","userTitle                    41       9   \n","userURL                       5       1   \n","inReplyTo              243832.0     NaN   \n","articleID                243832     886   \n","sectionName              243832      31   \n","newDesk                  243832      28   \n","articleWordCount       243832.0     NaN   \n","printPage              243832.0     NaN   \n","typeOfMaterial           243832      11   \n","\n","                                                                     top  \\\n","approveDate                                                          NaN   \n","commentBody                                                   Well said.   \n","commentID                                                            NaN   \n","commentSequence                                                      NaN   \n","commentTitle                                                       <br/>   \n","commentType                                                      comment   \n","createDate                                                           NaN   \n","depth                                                                NaN   \n","editorsSelection                                                   False   \n","parentID                                                             NaN   \n","parentUserDisplayName                                               John   \n","permID                                                          22022598   \n","picURL                 https://graphics8.nytimes.com/images/apps/time...   \n","recommendations                                                      NaN   \n","recommendedFlag                                                      NaN   \n","replyCount                                                           NaN   \n","reportAbuseFlag                                                      NaN   \n","sharing                                                              NaN   \n","status                                                          approved   \n","timespeople                                                          NaN   \n","trusted                                                              NaN   \n","updateDate                                                           NaN   \n","userDisplayName                                                     John   \n","userID                                                               NaN   \n","userLocation                                                         NYC   \n","userTitle                                           Your Money columnist   \n","userURL                http://www.nytimes.com/column/the-walking-dead...   \n","inReplyTo                                                            NaN   \n","articleID                                       58ebb1437c459f24986d96ed   \n","sectionName                                                      Unknown   \n","newDesk                                                             OpEd   \n","articleWordCount                                                     NaN   \n","printPage                                                            NaN   \n","typeOfMaterial                                                      News   \n","\n","                         freq               mean              std  \\\n","approveDate               NaN   1492504461.72283   1330667.064463   \n","commentBody                21                NaN              NaN   \n","commentID                 NaN    22188608.516839    185515.004384   \n","commentSequence           NaN    22188608.516839    185515.004384   \n","commentTitle           228498                NaN              NaN   \n","commentType            173277                NaN              NaN   \n","createDate                NaN  1492495336.431272   1328516.276157   \n","depth                     NaN           1.289425         0.453641   \n","editorsSelection       238159                NaN              NaN   \n","parentID                  NaN     6416284.403421  10055291.992253   \n","parentUserDisplayName     351                NaN              NaN   \n","permID                      1                NaN              NaN   \n","picURL                 201781                NaN              NaN   \n","recommendations           NaN          17.625172        93.219419   \n","recommendedFlag           NaN                NaN              NaN   \n","replyCount                NaN           0.405734         1.944378   \n","reportAbuseFlag           NaN                NaN              NaN   \n","sharing                   NaN           0.086371         0.280912   \n","status                 243832                NaN              NaN   \n","timespeople               NaN           0.999643         0.018886   \n","trusted                   NaN           0.033576         0.180136   \n","updateDate                NaN  1492505796.335194   1330736.099802   \n","userDisplayName          1214                NaN              NaN   \n","userID                    NaN     49557503.00269  23381533.206869   \n","userLocation             7292                NaN              NaN   \n","userTitle                  18                NaN              NaN   \n","userURL                     5                NaN              NaN   \n","inReplyTo                 NaN     6416193.792542  10055250.263762   \n","articleID                5181                NaN              NaN   \n","sectionName            137928                NaN              NaN   \n","newDesk                 82519                NaN              NaN   \n","articleWordCount          NaN        1293.667451       946.593427   \n","printPage                 NaN           9.458652        11.138473   \n","typeOfMaterial         130770                NaN              NaN   \n","\n","                                min            25%           50%  \\\n","approveDate            1491008047.0  1491783853.25  1492437034.5   \n","commentBody                     NaN            NaN           NaN   \n","commentID                21999548.0    22092910.75    22176681.5   \n","commentSequence          21999548.0    22092910.75    22176681.5   \n","commentTitle                    NaN            NaN           NaN   \n","commentType                     NaN            NaN           NaN   \n","createDate             1491006872.0  1491768493.75  1492428038.0   \n","depth                           1.0            1.0           1.0   \n","editorsSelection                NaN            NaN           NaN   \n","parentID                        0.0            0.0           0.0   \n","parentUserDisplayName           NaN            NaN           NaN   \n","permID                          NaN            NaN           NaN   \n","picURL                          NaN            NaN           NaN   \n","recommendations                 0.0            1.0           3.0   \n","recommendedFlag                 NaN            NaN           NaN   \n","replyCount                   -115.0            0.0           0.0   \n","reportAbuseFlag                 NaN            NaN           NaN   \n","sharing                         0.0            0.0           0.0   \n","status                          NaN            NaN           NaN   \n","timespeople                     0.0            1.0           1.0   \n","trusted                         0.0            0.0           0.0   \n","updateDate             1491008947.0   1491787044.0  1492437385.5   \n","userDisplayName                 NaN            NaN           NaN   \n","userID                       1045.0     32195826.0    57198758.0   \n","userLocation                    NaN            NaN           NaN   \n","userTitle                       NaN            NaN           NaN   \n","userURL                         NaN            NaN           NaN   \n","inReplyTo                       0.0            0.0           0.0   \n","articleID                       NaN            NaN           NaN   \n","sectionName                     NaN            NaN           NaN   \n","newDesk                         NaN            NaN           NaN   \n","articleWordCount               57.0          828.0        1124.0   \n","printPage                       0.0            1.0           2.0   \n","typeOfMaterial                  NaN            NaN           NaN   \n","\n","                                 75%           max  \n","approveDate             1493119511.0  1524346252.0  \n","commentBody                      NaN           NaN  \n","commentID                 22260467.5    26824246.0  \n","commentSequence           22260467.5    26824246.0  \n","commentTitle                     NaN           NaN  \n","commentType                      NaN           NaN  \n","createDate             1493086582.25  1524345694.0  \n","depth                            2.0           3.0  \n","editorsSelection                 NaN           NaN  \n","parentID                 22051084.75    26426201.0  \n","parentUserDisplayName            NaN           NaN  \n","permID                           NaN           NaN  \n","picURL                           NaN           NaN  \n","recommendations                  9.0        7938.0  \n","recommendedFlag                  NaN           NaN  \n","replyCount                       0.0         361.0  \n","reportAbuseFlag                  NaN           NaN  \n","sharing                          0.0           1.0  \n","status                           NaN           NaN  \n","timespeople                      1.0           1.0  \n","trusted                          0.0           1.0  \n","updateDate             1493122001.25  1524346252.0  \n","userDisplayName                  NaN           NaN  \n","userID                   67699953.75    85678080.0  \n","userLocation                     NaN           NaN  \n","userTitle                        NaN           NaN  \n","userURL                          NaN           NaN  \n","inReplyTo                22051083.25    26426201.0  \n","articleID                        NaN           NaN  \n","sectionName                      NaN           NaN  \n","newDesk                          NaN           NaN  \n","articleWordCount              1385.0        7832.0  \n","printPage                       20.0          74.0  \n","typeOfMaterial                   NaN           NaN  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"execution_count":12},{"cell_type":"markdown","source":"## **üìå Step 6: Check for Unique Identifiers**\nFind columns that can be used as unique identifiers.","metadata":{}},{"cell_type":"code","source":"# Check if any column can be used as a unique identifier\nunique_counts = df.nunique()\nunique_counts","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:06:22.070593Z","iopub.status.busy":"2025-02-28T17:06:22.070211Z","iopub.status.idle":"2025-02-28T17:06:22.083374Z","shell.execute_reply":"2025-02-28T17:06:22.082091Z","shell.execute_reply.started":"2025-02-28T17:06:22.070526Z"},"trusted":true},"outputs":[{"data":{"text/plain":["articleID           850\n","abstract             40\n","byline              434\n","documentType          2\n","headline            774\n","keywords            717\n","multimedia            2\n","newDesk              28\n","printPage            43\n","pubDate             786\n","sectionName          30\n","snippet             846\n","source                2\n","typeOfMaterial       11\n","webURL              850\n","articleWordCount    689\n","dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"execution_count":7},{"cell_type":"code","source":"# Check if any column can be used as a unique identifier\nunique_counts_2 = df_2.nunique()\nunique_counts_2","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:20:39.640657Z","iopub.status.busy":"2025-02-28T17:20:39.640315Z","iopub.status.idle":"2025-02-28T17:20:40.233436Z","shell.execute_reply":"2025-02-28T17:20:40.232445Z","shell.execute_reply.started":"2025-02-28T17:20:39.640631Z"},"trusted":true},"outputs":[{"data":{"text/plain":["approveDate              115718\n","commentBody              243169\n","commentID                243832\n","commentSequence          243832\n","commentTitle                  1\n","commentType                   3\n","createDate               228348\n","depth                         3\n","editorsSelection              2\n","parentID                  41494\n","parentUserDisplayName     15712\n","permID                   243832\n","picURL                     4282\n","recommendations            1232\n","recommendedFlag               0\n","replyCount                   79\n","reportAbuseFlag               0\n","sharing                       2\n","status                        1\n","timespeople                   2\n","trusted                       2\n","updateDate               136865\n","userDisplayName           46510\n","userID                    62946\n","userLocation              15890\n","userTitle                     9\n","userURL                       1\n","inReplyTo                 41494\n","articleID                   886\n","sectionName                  31\n","newDesk                      28\n","articleWordCount            680\n","printPage                    51\n","typeOfMaterial               11\n","dtype: int64"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"execution_count":13},{"cell_type":"markdown","source":"## **üìå Step 7: Automate Metadata Extraction for All Files**\nInstead of manually inspecting each file, we automate metadata extraction for all files.","metadata":{}},{"cell_type":"code","source":"# Iterate over all files and extract metadata\nmetadata_summary = []\n\nfor file in files:\n    file_path = os.path.join(dataset_path, file)\n    df = pd.read_csv(file_path)\n\n    metadata_summary.append({\n        \"File Name\": file,\n        \"Rows\": df.shape[0],\n        \"Columns\": df.shape[1],\n        \"Missing Values\": df.isnull().sum().sum(),\n        \"Duplicate Rows\": df.duplicated().sum(),\n        \"Unique Columns\": df.nunique().to_dict(),\n    })\n\n# Convert metadata summary to DataFrame for better readability\nmetadata_df = pd.DataFrame(metadata_summary)\nmetadata_df","metadata":{"execution":{"iopub.execute_input":"2025-02-28T17:08:12.662617Z","iopub.status.busy":"2025-02-28T17:08:12.662305Z","iopub.status.idle":"2025-02-28T17:08:59.232425Z","shell.execute_reply":"2025-02-28T17:08:59.230886Z","shell.execute_reply.started":"2025-02-28T17:08:12.662592Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (32) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (32) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (14,15,31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (14,15,31) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (14,15,31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n","<ipython-input-8-a0b18595323b>:6: DtypeWarning: Columns (32) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>File Name</th>\n","      <th>Rows</th>\n","      <th>Columns</th>\n","      <th>Missing Values</th>\n","      <th>Duplicate Rows</th>\n","      <th>Unique Columns</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CommentsFeb2018.csv</td>\n","      <td>215282</td>\n","      <td>34</td>\n","      <td>1018593</td>\n","      <td>0</td>\n","      <td>{'approveDate': 158054, 'articleID': 1155, 'ar...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ArticlesFeb2017.csv</td>\n","      <td>885</td>\n","      <td>16</td>\n","      <td>855</td>\n","      <td>0</td>\n","      <td>{'articleID': 885, 'abstract': 30, 'byline': 4...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CommentsApril2018.csv</td>\n","      <td>264924</td>\n","      <td>34</td>\n","      <td>1240919</td>\n","      <td>0</td>\n","      <td>{'approveDate': 196777, 'articleID': 1351, 'ar...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ArticlesJan2017.csv</td>\n","      <td>850</td>\n","      <td>16</td>\n","      <td>810</td>\n","      <td>0</td>\n","      <td>{'articleID': 850, 'abstract': 40, 'byline': 4...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ArticlesMay2017.csv</td>\n","      <td>996</td>\n","      <td>16</td>\n","      <td>963</td>\n","      <td>0</td>\n","      <td>{'abstract': 33, 'articleID': 996, 'articleWor...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>CommentsJan2017.csv</td>\n","      <td>231449</td>\n","      <td>34</td>\n","      <td>1114483</td>\n","      <td>0</td>\n","      <td>{'approveDate': 106710, 'articleID': 850, 'art...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CommentsMarch2017.csv</td>\n","      <td>260967</td>\n","      <td>34</td>\n","      <td>1249140</td>\n","      <td>0</td>\n","      <td>{'approveDate': 115903, 'articleID': 949, 'art...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>CommentsMay2017.csv</td>\n","      <td>276389</td>\n","      <td>34</td>\n","      <td>1322148</td>\n","      <td>0</td>\n","      <td>{'approveDate': 160236, 'commentBody': 275493,...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>CommentsMarch2018.csv</td>\n","      <td>246915</td>\n","      <td>34</td>\n","      <td>1331416</td>\n","      <td>0</td>\n","      <td>{'approveDate': 187256, 'articleID': 1385, 'ar...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>CommentsApril2017.csv</td>\n","      <td>243832</td>\n","      <td>34</td>\n","      <td>1164061</td>\n","      <td>0</td>\n","      <td>{'approveDate': 115718, 'commentBody': 243169,...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>ArticlesMarch2017.csv</td>\n","      <td>949</td>\n","      <td>16</td>\n","      <td>918</td>\n","      <td>0</td>\n","      <td>{'abstract': 30, 'articleID': 949, 'articleWor...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>ArticlesApril2017.csv</td>\n","      <td>886</td>\n","      <td>16</td>\n","      <td>864</td>\n","      <td>0</td>\n","      <td>{'abstract': 22, 'articleID': 886, 'articleWor...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>CommentsFeb2017.csv</td>\n","      <td>233407</td>\n","      <td>34</td>\n","      <td>1128697</td>\n","      <td>0</td>\n","      <td>{'approveDate': 100268, 'articleID': 885, 'art...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>ArticlesJan2018.csv</td>\n","      <td>905</td>\n","      <td>16</td>\n","      <td>894</td>\n","      <td>0</td>\n","      <td>{'abstract': 11, 'articleID': 905, 'articleWor...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>ArticlesFeb2018.csv</td>\n","      <td>1155</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>{'articleID': 1155, 'byline': 650, 'documentTy...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>ArticlesMarch2018.csv</td>\n","      <td>1385</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>{'articleID': 1385, 'byline': 783, 'documentTy...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>CommentsJan2018.csv</td>\n","      <td>203199</td>\n","      <td>34</td>\n","      <td>959348</td>\n","      <td>0</td>\n","      <td>{'approveDate': 148322, 'articleID': 1030, 'ar...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>ArticlesApril2018.csv</td>\n","      <td>1324</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>{'articleID': 1324, 'articleWordCount': 934, '...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                File Name    Rows  Columns  Missing Values  Duplicate Rows  \\\n","0     CommentsFeb2018.csv  215282       34         1018593               0   \n","1     ArticlesFeb2017.csv     885       16             855               0   \n","2   CommentsApril2018.csv  264924       34         1240919               0   \n","3     ArticlesJan2017.csv     850       16             810               0   \n","4     ArticlesMay2017.csv     996       16             963               0   \n","5     CommentsJan2017.csv  231449       34         1114483               0   \n","6   CommentsMarch2017.csv  260967       34         1249140               0   \n","7     CommentsMay2017.csv  276389       34         1322148               0   \n","8   CommentsMarch2018.csv  246915       34         1331416               0   \n","9   CommentsApril2017.csv  243832       34         1164061               0   \n","10  ArticlesMarch2017.csv     949       16             918               0   \n","11  ArticlesApril2017.csv     886       16             864               0   \n","12    CommentsFeb2017.csv  233407       34         1128697               0   \n","13    ArticlesJan2018.csv     905       16             894               0   \n","14    ArticlesFeb2018.csv    1155       15               0               0   \n","15  ArticlesMarch2018.csv    1385       15               0               0   \n","16    CommentsJan2018.csv  203199       34          959348               0   \n","17  ArticlesApril2018.csv    1324       15               0               0   \n","\n","                                       Unique Columns  \n","0   {'approveDate': 158054, 'articleID': 1155, 'ar...  \n","1   {'articleID': 885, 'abstract': 30, 'byline': 4...  \n","2   {'approveDate': 196777, 'articleID': 1351, 'ar...  \n","3   {'articleID': 850, 'abstract': 40, 'byline': 4...  \n","4   {'abstract': 33, 'articleID': 996, 'articleWor...  \n","5   {'approveDate': 106710, 'articleID': 850, 'art...  \n","6   {'approveDate': 115903, 'articleID': 949, 'art...  \n","7   {'approveDate': 160236, 'commentBody': 275493,...  \n","8   {'approveDate': 187256, 'articleID': 1385, 'ar...  \n","9   {'approveDate': 115718, 'commentBody': 243169,...  \n","10  {'abstract': 30, 'articleID': 949, 'articleWor...  \n","11  {'abstract': 22, 'articleID': 886, 'articleWor...  \n","12  {'approveDate': 100268, 'articleID': 885, 'art...  \n","13  {'abstract': 11, 'articleID': 905, 'articleWor...  \n","14  {'articleID': 1155, 'byline': 650, 'documentTy...  \n","15  {'articleID': 1385, 'byline': 783, 'documentTy...  \n","16  {'approveDate': 148322, 'articleID': 1030, 'ar...  \n","17  {'articleID': 1324, 'articleWordCount': 934, '...  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"execution_count":8},{"cell_type":"markdown","source":"## **üîç Conclusion**\nThis notebook provides insights into the dataset structure, missing values, and metadata, making it ready for further text processing and LSTM-based text generation analysis.","metadata":{}},{"cell_type":"markdown","source":"# üìä **LSTM-Based Text Generation on NYT Comments Dataset**\nThis notebook trains an LSTM model using the **New York Times Comments dataset** to generate human-like text. The notebook follows a structured process: merging datasets, preprocessing text, tokenization, training an LSTM model, and saving progress to prevent data loss in case of session shutdown.","metadata":{}},{"cell_type":"markdown","source":"## **üìå Step 1: Load & Merge All Comment Datasets**\nWe combine all comments into a single dataset for better model generalization.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Path to dataset directory\ndataset_path = \"/kaggle/input/nyt-comments\"\n\n# List all comment files\ncomment_files = [file for file in os.listdir(dataset_path) if file.startswith(\"Comments\")]\n\n# Initialize empty list to store DataFrames\ndf_list = []\n\n# Load and merge all comment files\nfor file in comment_files:\n    file_path = os.path.join(dataset_path, file)\n    df = pd.read_csv(file_path, usecols=[\"commentBody\"])\n    df_list.append(df)\n\n# Combine all comments into one DataFrame\ndf_combined = pd.concat(df_list, ignore_index=True)\n\n# Save merged dataset to avoid reloading\ndf_combined.to_csv(\"nyt_comments_cleaned.csv\", index=False)\n\n# Display dataset shape\nprint(\"Total Comments:\", df_combined.shape[0])\ndf_combined.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:25:29.466456Z","iopub.execute_input":"2025-02-28T18:25:29.466783Z","iopub.status.idle":"2025-02-28T18:26:07.503687Z","shell.execute_reply.started":"2025-02-28T18:25:29.466758Z","shell.execute_reply":"2025-02-28T18:26:07.502803Z"}},"outputs":[{"name":"stdout","text":"Total Comments: 2176364\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                         commentBody\n0  The snake-filled heads comment made me think o...\n1                      She-devil reporting for duty!\n2                   XX is the new mark of the devil.\n3  \"Courtland Sykes\" should be writing for The On...\n4  I happen to descend for a few of them, because...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>commentBody</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The snake-filled heads comment made me think o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>She-devil reporting for duty!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XX is the new mark of the devil.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"Courtland Sykes\" should be writing for The On...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I happen to descend for a few of them, because...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## **üìå Step 2: Preprocessing the Text**\nWe clean the text by converting to lowercase, removing special characters, and tokenizing words into sequences.","metadata":{}},{"cell_type":"markdown","source":"### The implemntation below uses too much memory","metadata":{}},{"cell_type":"code","source":"import re\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntqdm.pandas()  # Enables progress bars for Pandas operations\n\n# Function to clean text with tqdm progress bar\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n    return text\n\n# Apply text cleaning with a progress bar\nprint(\"\\nüîÑ Cleaning text data...\")\ndf_combined[\"commentBody\"] = df_combined[\"commentBody\"].astype(str).progress_apply(clean_text)\n\n# ‚úÖ Print sample cleaned text\nprint(\"\\nüìå Sample cleaned text:\\n\", df_combined[\"commentBody\"].head(5))\n\n# Tokenize text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_combined[\"commentBody\"])\n\n# ‚úÖ Print vocabulary size\nprint(f\"\\nüìå Vocabulary Size: {len(tokenizer.word_index)} unique words\")\n\n# Save tokenizer\nwith open(\"tokenizer.json\", \"w\") as f:\n    json.dump(tokenizer.to_json(), f)\n\n# Convert text to sequences\nprint(\"\\nüîÑ Tokenizing text sequences...\")\nsequences = tokenizer.texts_to_sequences(df_combined[\"commentBody\"])\n\n# ‚úÖ Print first few tokenized sequences\nprint(\"\\nüìå Sample tokenized sequences:\\n\", sequences[:5])\n\n# Create input sequences with tqdm progress bar\nsequence_length = 50\ninput_sequences = []\n\nprint(\"\\nüîÑ Creating input sequences...\")\nfor seq in tqdm(sequences, desc=\"Processing sequences\"):\n    for i in range(1, len(seq)):\n        input_sequences.append(seq[:i+1])\n\n# ‚úÖ Print first few input sequences before padding\nprint(\"\\nüìå Sample input sequences before padding:\\n\", input_sequences[:5])\n\n# Pad sequences\nprint(\"\\nüîÑ Padding input sequences...\")\ninput_sequences = pad_sequences(input_sequences, maxlen=sequence_length, padding=\"pre\")\n\n# ‚úÖ Print shape of input sequences after padding\nprint(f\"\\nüìå Padded input shape: {input_sequences.shape}\")\n\n# Extract input (X) and output (y)\nX, y = input_sequences[:, :-1], input_sequences[:, -1]\n\n# ‚úÖ Print shape of X and y\nprint(f\"\\nüìå X shape: {X.shape}, y shape: {y.shape}\")\n\n# Convert y to categorical\nprint(\"\\nüîÑ One-hot encoding target labels...\")\ny = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n\n# ‚úÖ Print a sample one-hot encoded output\nprint(\"\\nüìå Sample y (one-hot encoded output):\\n\", y[:3])\n\n# Save tokenized sequences\nprint(\"\\nüíæ Saving tokenized sequences...\")\nnp.save(\"input_sequences.npy\", input_sequences)\n\n# ‚úÖ Confirm data saving\nprint(\"\\n‚úÖ Tokenized sequences saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:14:13.339473Z","iopub.execute_input":"2025-02-28T18:14:13.339861Z","execution_failed":"2025-02-28T18:20:47.324Z"}},"outputs":[{"name":"stdout","text":"\nüîÑ Cleaning text data...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2176364/2176364 [01:06<00:00, 32676.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüìå Sample cleaned text:\n 0    the snakefilled heads comment made me think of...\n1                          shedevil reporting for duty\n2                      xx is the new mark of the devil\n3    courtland sykes should be writing for the onio...\n4    i happen to descend for a few of them because ...\nName: commentBody, dtype: object\n\nüìå Vocabulary Size: 1525475 unique words\n\nüîÑ Tokenizing text sequences...\n\nüìå Sample tokenized sequences:\n [[1, 151481, 1639, 624, 167, 84, 83, 4, 51071, 633, 212, 22, 3913, 41, 13206], [66071, 1181, 9, 1837], [22001, 6, 1, 116, 1946, 4, 1, 3835], [64185, 12872, 64, 15, 884, 9, 1, 11959, 13, 626, 9, 236], [10, 459, 2, 11706, 9, 5, 233, 4, 59, 69, 23, 2758, 168, 53, 2, 15706, 7, 11843, 49511, 9723, 3, 423138, 161, 425, 43, 301, 29, 622, 2, 15, 15318, 1010, 49511, 55, 1748, 98, 1, 253, 27, 23, 362, 101, 50, 4, 51]]\n\nüîÑ Creating input sequences...\n","output_type":"stream"},{"name":"stderr","text":"Processing sequences:  30%|‚ñà‚ñà‚ñâ       | 650081/2176364 [02:23<03:27, 7360.24it/s] ","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Will now implement an more memory friendly solution","metadata":{}},{"cell_type":"code","source":"import re\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntqdm.pandas()\n\n# Function to clean text\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)  \n    text = re.sub(r'[^\\w\\s]', '', text)  \n    text = re.sub(r'\\s+', ' ', text).strip()  \n    return text\n\n# Apply text cleaning with progress bar\nprint(\"\\nüîÑ Cleaning text data...\")\ndf_combined[\"commentBody\"] = df_combined[\"commentBody\"].astype(str).progress_apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:37:31.908795Z","iopub.execute_input":"2025-02-28T18:37:31.909099Z","iopub.status.idle":"2025-02-28T18:38:33.999924Z","shell.execute_reply.started":"2025-02-28T18:37:31.909077Z","shell.execute_reply":"2025-02-28T18:38:33.998873Z"}},"outputs":[{"name":"stdout","text":"\nüîÑ Cleaning text data...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2176364/2176364 [01:01<00:00, 35259.67it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Tokenize text with vocab limit\ntokenizer = Tokenizer(num_words=20000)  # Limit vocab to 20,000 words\n\nprint(\"\\nüîÑ Fitting tokenizer on text data...\")\ntokenizer.fit_on_texts(tqdm(df_combined[\"commentBody\"], desc=\"Processing text\"))\n\n# ‚úÖ Print unique words before filtering\nprint(f\"\\nüìå Total Unique Words (before limiting): {len(tokenizer.word_index)}\")\n\n# ‚úÖ Extract word frequencies\nword_counts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n\n# ‚úÖ Keep only the most frequent 20,000 words\ntokenizer.word_index = {word: i for word, i in word_counts[:20000]}\n\n# ‚úÖ Verify Vocabulary Size\nprint(f\"\\nüìå Vocabulary Size (After Limiting): {len(tokenizer.word_index)} words\")\n\n# ‚úÖ Print top 10 most frequent words\nprint(\"\\nüìå Top 10 Most Frequent Words in the Dataset:\")\nfor i, (word, count) in enumerate(word_counts[:10]):\n    print(f\"   {i+1}. {word}: {count} occurrences\")\n\n# Save tokenizer\nwith open(\"tokenizer.json\", \"w\") as f:\n    json.dump(tokenizer.to_json(), f)\n\n# Convert text to sequences with tqdm progress bar\nprint(\"\\nüîÑ Tokenizing text sequences...\")\nsequences = list(tqdm(tokenizer.texts_to_sequences(df_combined[\"commentBody\"]), desc=\"Converting to sequences\"))\n\n# ‚úÖ Confirm the process is complete\nprint(\"\\n‚úÖ Tokenization complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:40:26.179456Z","iopub.execute_input":"2025-02-28T18:40:26.179856Z","iopub.status.idle":"2025-02-28T18:43:20.621464Z","shell.execute_reply.started":"2025-02-28T18:40:26.179827Z","shell.execute_reply":"2025-02-28T18:43:20.620345Z"}},"outputs":[{"name":"stdout","text":"\nüîÑ Fitting tokenizer on text data...\n","output_type":"stream"},{"name":"stderr","text":"Processing text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2176364/2176364 [01:45<00:00, 20662.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüìå Total Unique Words (before limiting): 1525475\n\nüìå Vocabulary Size (After Limiting): 20000 words\n\nüìå Top 10 Most Frequent Words in the Dataset:\n   1. the: 8013972 occurrences\n   2. to: 4570214 occurrences\n   3. and: 4198158 occurrences\n   4. of: 3740329 occurrences\n   5. a: 3271858 occurrences\n   6. is: 2604472 occurrences\n   7. in: 2367733 occurrences\n   8. that: 2219162 occurrences\n   9. for: 1606318 occurrences\n   10. i: 1387537 occurrences\n\nüîÑ Tokenizing text sequences...\n","output_type":"stream"},{"name":"stderr","text":"Converting to sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2176364/2176364 [00:00<00:00, 4614921.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Tokenization complete!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Create input sequences using NumPy arrays (Optimized)\nsequence_length = 30  # Reduced from 50 to 30\nmax_sequences = sum(len(seq) for seq in sequences)\ninput_sequences = np.zeros((max_sequences, sequence_length), dtype=np.int32)\n\n\nprint(\"\\nüîÑ Creating input sequences...\")\nindex = 0\nfor seq in tqdm(sequences, desc=\"Processing sequences\"):\n    for i in range(1, len(seq)):\n        sub_seq = seq[:i+1]\n\n        # ‚úÖ Fix: Trim sequences that exceed `sequence_length`\n        if len(sub_seq) > sequence_length:\n            sub_seq = sub_seq[-sequence_length:]  # Keep last 30 tokens\n        \n        input_sequences[index, -len(sub_seq):] = sub_seq  # Insert at the end\n        index += 1\n\n# Pad sequences with reduced max length\nprint(\"\\nüîÑ Padding input sequences...\")\ninput_sequences = pad_sequences(input_sequences, maxlen=sequence_length, padding=\"pre\")\n\nprint(f\"\\nüìå Padded input shape: {input_sequences.shape}\")\n\n# Extract input (X) and output (y)\nX, y = input_sequences[:, :-1], input_sequences[:, -1]\n\nprint(f\"\\nüìå X shape: {X.shape}, y shape: {y.shape}\")\n\n# Convert y to integer labels (Sparse Encoding)\nprint(\"\\nüîÑ Converting y to sparse labels...\")\ny = np.array(y, dtype=np.int32)  # Uses sparse categorical encoding\n\n# Save tokenized sequences\nprint(\"\\nüíæ Saving tokenized sequences...\")\nnp.save(\"input_sequences.npy\", input_sequences)\n\nprint(\"\\n‚úÖ Tokenized sequences saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:44:37.749142Z","iopub.execute_input":"2025-02-28T18:44:37.749435Z","iopub.status.idle":"2025-02-28T18:46:26.639197Z","shell.execute_reply.started":"2025-02-28T18:44:37.749411Z","shell.execute_reply":"2025-02-28T18:46:26.638163Z"}},"outputs":[{"name":"stdout","text":"\nüîÑ Creating input sequences...\n","output_type":"stream"},{"name":"stderr","text":"Processing sequences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2176364/2176364 [00:54<00:00, 40274.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÑ Padding input sequences...\n\nüìå Padded input shape: (37039301, 30)\n\nüìå X shape: (37039301, 29), y shape: (37039301,)\n\nüîÑ Converting y to sparse labels...\n\nüíæ Saving tokenized sequences...\n\n‚úÖ Tokenized sequences saved successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## **üìå Step 3: Building the LSTM Model**\nWe define an LSTM-based architecture with embedding and dense layers.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nsequence_length = 30  # Reduced from 50 to 30\n\n# ‚úÖ Load preprocessed input sequences\nprint(\"\\nüìÇ Loading preprocessed sequences from 'input_sequences.npy'...\")\ninput_sequences = np.load(\"input_sequences.npy\", allow_pickle=True)\n\n# ‚úÖ Extract X and y from input sequences\nX, y = input_sequences[:, :-1], input_sequences[:, -1]\n\nprint(f\"‚úÖ Loaded input sequences. X shape: {X.shape}, y shape: {y.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:06:01.742297Z","iopub.execute_input":"2025-02-28T20:06:01.742602Z","iopub.status.idle":"2025-02-28T20:06:11.186553Z","shell.execute_reply.started":"2025-02-28T20:06:01.742582Z","shell.execute_reply":"2025-02-28T20:06:11.185650Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Loading preprocessed sequences from 'input_sequences.npy'...\n‚úÖ Loaded input sequences. X shape: (37039301, 29), y shape: (37039301,)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\nfrom tensorflow.keras.preprocessing.text import tokenizer_from_json\n\n# ‚úÖ Load tokenizer\nprint(\"\\nüìÇ Loading tokenizer from 'tokenizer.json'...\")\nwith open(\"tokenizer.json\", \"r\") as f:\n    tokenizer = tokenizer_from_json(json.load(f))\n\n# ‚úÖ Get vocab size\nvocab_size = len(tokenizer.word_index) + 1\nprint(f\"‚úÖ Loaded tokenizer. Vocabulary size: {vocab_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:06:11.187719Z","iopub.execute_input":"2025-02-28T20:06:11.188018Z","iopub.status.idle":"2025-02-28T20:06:31.889228Z","shell.execute_reply.started":"2025-02-28T20:06:11.187986Z","shell.execute_reply":"2025-02-28T20:06:31.888301Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Loading tokenizer from 'tokenizer.json'...\n‚úÖ Loaded tokenizer. Vocabulary size: 20001\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n\n# Ensure tokenizer has been trained\nif not tokenizer.word_index:\n    raise ValueError(\"Tokenizer word_index is empty. Ensure tokenizer.fit_on_texts() was called.\")\n\n# Define vocabulary size\nvocab_size = len(tokenizer.word_index) + 1  # Ensure vocabulary size is correct\n\n# ‚úÖ Use `Input()` for defining the input layer\nmodel = Sequential([\n    Input(shape=(sequence_length-1,)),  # Explicit input layer\n    Embedding(input_dim=vocab_size, output_dim=128),  # Removed input_shape\n    LSTM(128, return_sequences=True),\n    LSTM(128),\n    Dense(128, activation=\"relu\"),\n    Dense(vocab_size, activation=\"softmax\")\n])\n\n# Compile model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# ‚úÖ Print model summary (No need to call `build()` manually)\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:06:31.890981Z","iopub.execute_input":"2025-02-28T20:06:31.891532Z","iopub.status.idle":"2025-02-28T20:06:34.179309Z","shell.execute_reply.started":"2025-02-28T20:06:31.891507Z","shell.execute_reply":"2025-02-28T20:06:34.178611Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m128\u001b[0m)             ‚îÇ       \u001b[38;5;34m2,560,128\u001b[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m128\u001b[0m)             ‚îÇ         \u001b[38;5;34m131,584\u001b[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ         \u001b[38;5;34m131,584\u001b[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ          \u001b[38;5;34m16,512\u001b[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20001\u001b[0m)               ‚îÇ       \u001b[38;5;34m2,580,129\u001b[0m ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\"> Layer (type)                         </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape                </span>‚îÉ<span style=\"font-weight: bold\">         Param # </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,128</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20001</span>)               ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,129</span> ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,419,937\u001b[0m (20.68 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,419,937</span> (20.68 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,419,937\u001b[0m (20.68 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,419,937</span> (20.68 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## **üìå Step 4: Training the LSTM Model**\nWe train the LSTM model with categorical cross-entropy loss.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nprint(\"\\nüìÇ Loading cleaned dataset 'nyt_comments_cleaned.csv'...\")\ndf_combined = pd.read_csv(\"nyt_comments_cleaned.csv\")\n\nprint(f\"‚úÖ Loaded dataset with {df_combined.shape[0]} rows.\")\nprint(df_combined.head())  # Preview first few rows\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:09:10.335458Z","iopub.execute_input":"2025-02-28T19:09:10.336068Z","iopub.status.idle":"2025-02-28T19:09:21.280763Z","shell.execute_reply.started":"2025-02-28T19:09:10.336035Z","shell.execute_reply":"2025-02-28T19:09:21.279941Z"}},"outputs":[{"name":"stdout","text":"\nüìÇ Loading cleaned dataset 'nyt_comments_cleaned.csv'...\n‚úÖ Loaded dataset with 2176364 rows.\n                                         commentBody\n0  The snake-filled heads comment made me think o...\n1                      She-devil reporting for duty!\n2                   XX is the new mark of the devil.\n3  \"Courtland Sykes\" should be writing for The On...\n4  I happen to descend for a few of them, because...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check if GPU is available\nprint(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:06:47.965908Z","iopub.execute_input":"2025-02-28T20:06:47.966240Z","iopub.status.idle":"2025-02-28T20:06:47.971188Z","shell.execute_reply.started":"2025-02-28T20:06:47.966213Z","shell.execute_reply":"2025-02-28T20:06:47.970299Z"}},"outputs":[{"name":"stdout","text":"Num GPUs Available: 1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Ensure TensorFlow uses GPU\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)  # Prevents memory overflow issues\n        print(\"\\n‚úÖ GPU is enabled and TensorFlow is using it!\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"\\n‚ùå No GPU detected, training may be slow!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:06:49.234400Z","iopub.execute_input":"2025-02-28T20:06:49.234689Z","iopub.status.idle":"2025-02-28T20:06:49.239926Z","shell.execute_reply.started":"2025-02-28T20:06:49.234669Z","shell.execute_reply":"2025-02-28T20:06:49.239046Z"}},"outputs":[{"name":"stdout","text":"Physical devices cannot be modified after being initialized\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import time\nimport pickle\nimport numpy as np\n\n# Ensure `y` is sparse categorical (integer labels)\ny = np.array(y, dtype=np.int32)\n\n# Compile model with sparse categorical cross-entropy\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# Start Timer\nstart_time = time.time()\nprint(\"\\nüöÄ Starting Model Training...\\n\")\n\n# Train model with verbose logging\nhistory = model.fit(\n    X, y,\n    epochs=10,\n    batch_size=512,\n    validation_split=0.2,\n    verbose=1\n)\n\n# Compute total training time\nend_time = time.time()\ntotal_time = end_time - start_time\nprint(f\"\\n‚úÖ Training Completed in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n\n# Save Model\nmodel.save(\"nyt_lstm_model.h5\")\nprint(\"\\nüíæ Model saved as 'nyt_lstm_model.h5'\")\n\n# Save Training History\nwith open(\"training_history.pkl\", \"wb\") as f:\n    pickle.dump(history.history, f)\nprint(\"\\nüìä Training history saved as 'training_history.pkl'\")\n\n# Print Final Training Stats\nprint(\"\\nüìå Final Training Metrics:\")\nprint(f\"   üîπ Final Training Loss: {history.history['loss'][-1]:.4f}\")\nprint(f\"   üîπ Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\nprint(f\"   üîπ Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\nprint(f\"   üîπ Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n\nprint(\"\\nüéØ Training Complete! You can now evaluate the model and generate text.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T20:08:05.648987Z","iopub.execute_input":"2025-02-28T20:08:05.649309Z","iopub.status.idle":"2025-02-28T20:09:17.006485Z","shell.execute_reply.started":"2025-02-28T20:08:05.649286Z","shell.execute_reply":"2025-02-28T20:09:17.005151Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Starting Model Training...\n\nEpoch 1/10\n\u001b[1m 1521/57874\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m38:15\u001b[0m 41ms/step - accuracy: 9.2576e-04 - loss: 8.4111","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-28e3c7084887>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Train model with verbose logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"## **üìå Step 5: Generate New Comments Using the LSTM**\nWe use the trained model to predict and generate text from a given seed phrase.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef generate_text(seed_text, next_words=50, temperature=1.0):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=sequence_length-1, padding=\"pre\")\n\n        # Predict next word\n        predicted_probs = model.predict(token_list, verbose=0)\n        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n\n        # Convert index to word\n        output_word = tokenizer.index_word.get(predicted_index, \"\")\n        seed_text += \" \" + output_word\n    return seed_text\n\n# Example\nprint(generate_text(\"the government should\", next_words=20))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **üìå Final Summary**\n1. **Merged all comment datasets** into a single dataset.\n2. **Preprocessed and tokenized the text** for input sequences.\n3. **Trained an LSTM model** with embeddings and dense layers.\n4. **Saved progress at every stage** to prevent data loss.\n5. **Generated new comments** based on seed text input.","metadata":{}}]}