{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä **New York Times Comments Dataset Analysis**\n",
    "This notebook analyzes the New York Times Comments dataset available on Kaggle.\n",
    "We will extract metadata, check for missing values, and summarize the structure of the dataset before proceeding with text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 1: Setup the Environment**\n",
    "We start by importing the necessary libraries and listing all available files in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set path to dataset (Kaggle users should adjust as needed)\n",
    "dataset_path = \"../input/nyt-comments/\"\n",
    "\n",
    "# List all files in the dataset\n",
    "files = os.listdir(dataset_path)\n",
    "print(\"Files in dataset:\\n\", files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 2: Load & Inspect Data**\n",
    "Let's load one file (e.g., `ArticlesJan2017.csv`) to inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example file to inspect its structure\n",
    "sample_file = \"ArticlesJan2017.csv\"  # You can change this to any file in the dataset\n",
    "df = pd.read_csv(os.path.join(dataset_path, sample_file))\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 3: Extract Metadata**\n",
    "Now, we extract key metadata, such as column names, data types, and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 4: Check for Missing Values**\n",
    "Checking for missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 5: Summary Statistics**\n",
    "Generate a summary of numeric and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "df.describe(include=\"all\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 6: Check for Unique Identifiers**\n",
    "Find columns that can be used as unique identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any column can be used as a unique identifier\n",
    "unique_counts = df.nunique()\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 7: Automate Metadata Extraction for All Files**\n",
    "Instead of manually inspecting each file, we automate metadata extraction for all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all files and extract metadata\n",
    "metadata_summary = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    metadata_summary.append({\n",
    "        \"File Name\": file,\n",
    "        \"Rows\": df.shape[0],\n",
    "        \"Columns\": df.shape[1],\n",
    "        \"Missing Values\": df.isnull().sum().sum(),\n",
    "        \"Duplicate Rows\": df.duplicated().sum(),\n",
    "        \"Unique Columns\": df.nunique().to_dict(),\n",
    "    })\n",
    "\n",
    "# Convert metadata summary to DataFrame for better readability\n",
    "metadata_df = pd.DataFrame(metadata_summary)\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üîç Conclusion**\n",
    "This notebook provides insights into the dataset structure, missing values, and metadata, making it ready for further text processing and LSTM-based text generation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä **LSTM-Based Text Generation on NYT Comments Dataset**\n",
    "This notebook trains an LSTM model using the **New York Times Comments dataset** to generate human-like text. The notebook follows a structured process: merging datasets, preprocessing text, tokenization, training an LSTM model, and saving progress to prevent data loss in case of session shutdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 1: Load & Merge All Comment Datasets**\n",
    "We combine all comments into a single dataset for better model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to dataset directory\n",
    "dataset_path = \"../input/nyt-comments/\"\n",
    "\n",
    "# List all comment files\n",
    "comment_files = [file for file in os.listdir(dataset_path) if file.startswith(\"Comments\")]\n",
    "\n",
    "# Initialize empty list to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Load and merge all comment files\n",
    "for file in comment_files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    df = pd.read_csv(file_path, usecols=[\"commentBody\"])\n",
    "    df_list.append(df)\n",
    "\n",
    "# Combine all comments into one DataFrame\n",
    "df_combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save merged dataset to avoid reloading\n",
    "df_combined.to_csv(\"nyt_comments_cleaned.csv\", index=False)\n",
    "\n",
    "# Display dataset shape\n",
    "print(\"Total Comments:\", df_combined.shape[0])\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 2: Preprocessing the Text**\n",
    "We clean the text by converting to lowercase, removing special characters, and tokenizing words into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df_combined[\"commentBody\"] = df_combined[\"commentBody\"].astype(str).apply(clean_text)\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_combined[\"commentBody\"])\n",
    "\n",
    "# Save tokenizer\n",
    "with open(\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df_combined[\"commentBody\"])\n",
    "\n",
    "# Create input sequences\n",
    "sequence_length = 50\n",
    "input_sequences = []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        input_sequences.append(seq[:i+1])\n",
    "\n",
    "# Pad sequences\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=sequence_length, padding=\"pre\")\n",
    "\n",
    "# Extract input (X) and output (y)\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# Convert y to categorical\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Save tokenized sequences\n",
    "import numpy as np\n",
    "np.save(\"input_sequences.npy\", input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 3: Building the LSTM Model**\n",
    "We define an LSTM-based architecture with embedding and dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define LSTM model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 128, input_length=sequence_length-1),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    LSTM(256),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 4: Training the LSTM Model**\n",
    "We train the LSTM model with categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(X, y, epochs=30, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Save model\n",
    "model.save(\"nyt_lstm_model.h5\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open(\"training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 5: Generate New Comments Using the LSTM**\n",
    "We use the trained model to predict and generate text from a given seed phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_text(seed_text, next_words=50, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=sequence_length-1, padding=\"pre\")\n",
    "\n",
    "        # Predict next word\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
    "\n",
    "        # Convert index to word\n",
    "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Example\n",
    "print(generate_text(\"the government should\", next_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Final Summary**\n",
    "1. **Merged all comment datasets** into a single dataset.\n",
    "2. **Preprocessed and tokenized the text** for input sequences.\n",
    "3. **Trained an LSTM model** with embeddings and dense layers.\n",
    "4. **Saved progress at every stage** to prevent data loss.\n",
    "5. **Generated new comments** based on seed text input."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
