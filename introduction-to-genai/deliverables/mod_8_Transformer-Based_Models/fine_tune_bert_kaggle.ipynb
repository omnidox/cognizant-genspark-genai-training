{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c62d92e9","cell_type":"markdown","source":"# **Fine-Tuning BERT with Hugging Face**\n### **Kaggle Notebook**\nAuthor: *Rafael Hidalgo*  \nDate: *03/02/2025*  \n\n## **1. Introduction**\nThis notebook demonstrates how to fine-tune a BERT model for sentiment analysis using the IMDb dataset. We will use Hugging Face's `transformers` and `datasets` libraries to:\n- Preprocess and tokenize the dataset\n- Train a BERT model for text classification\n- Debug and optimize training performance\n- Evaluate the fine-tuned model using key metrics\n- Explore potential real-world applications\n","metadata":{}},{"id":"6767b446","cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:30:20.632717Z","iopub.execute_input":"2025-03-03T01:30:20.633079Z","iopub.status.idle":"2025-03-03T01:30:25.241936Z","shell.execute_reply.started":"2025-03-03T01:30:20.633052Z","shell.execute_reply":"2025-03-03T01:30:25.240771Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"id":"0c705c40","cell_type":"code","source":"# Import necessary libraries\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:30:29.637933Z","iopub.execute_input":"2025-03-03T01:30:29.638241Z","iopub.status.idle":"2025-03-03T01:30:50.128503Z","shell.execute_reply.started":"2025-03-03T01:30:29.638214Z","shell.execute_reply":"2025-03-03T01:30:50.127648Z"}},"outputs":[],"execution_count":4},{"id":"bf161968","cell_type":"markdown","source":"## **3. Load and Prepare the IMDb Dataset**","metadata":{}},{"id":"05d6e38d","cell_type":"code","source":"# Load dataset\ndataset = load_dataset('imdb')\n\n# Load tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the data\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n\n# Apply tokenization\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Rename the label column\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n\n# Convert dataset to PyTorch format\ntokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# Subset the dataset for quick training\ntrain_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\ntest_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:30:50.129707Z","iopub.execute_input":"2025-03-03T01:30:50.130267Z","iopub.status.idle":"2025-03-03T01:39:56.392263Z","shell.execute_reply.started":"2025-03-03T01:30:50.130244Z","shell.execute_reply":"2025-03-03T01:39:56.391564Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc88005cddd41e4aef86d8c837e9ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc33cbcfa554221add60652ec56ac96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4270dc856cd44e7b97b5851426c537bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e244b0bf3642ffb0a106834f489e61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20235a47ae2846c39426146db5124400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9c77b87a62432595c1a7240cd6af6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2cd86b45954676890435a55e11cb06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df9333dab162437eb6b9564ddbc342ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02ef7411203649e1a7dfeb690d0faf9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d35e0e9c5947ffa8bfa40a4c1eba1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b4c3c273ec485a8d0ee3563a177eaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe06f11e36c43abb602f1bbfe2dc026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c5d571b77f4869b127521cd0d9ffd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b6796caa9d4ae2994486c20d6360d1"}},"metadata":{}}],"execution_count":5},{"id":"0a29f8ee","cell_type":"markdown","source":"## **4. Load Pre-Trained BERT Model**","metadata":{}},{"id":"d2bf0d2a","cell_type":"code","source":"# Load pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:39:56.393993Z","iopub.execute_input":"2025-03-03T01:39:56.394218Z","iopub.status.idle":"2025-03-03T01:40:02.827145Z","shell.execute_reply.started":"2025-03-03T01:39:56.394199Z","shell.execute_reply":"2025-03-03T01:40:02.826208Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8151013258b645bfa94814b12e6a0847"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"id":"08971716","cell_type":"markdown","source":"## **5. Define Training Arguments**","metadata":{}},{"id":"f8c2b7fd","cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"steps\",  # Ensures logs appear at each step\n    logging_steps=10,  # Log every 10 steps\n    save_strategy=\"epoch\",  # Save model checkpoints at every epoch\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    save_total_limit=2,  # Keeps only the last 2 checkpoints\n    report_to=\"none\",  # Prevents logging to external platforms like TensorBoard\n    fp16=True,  # Enables mixed precision training on GPU\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:40:02.828118Z","iopub.execute_input":"2025-03-03T01:40:02.828350Z","iopub.status.idle":"2025-03-03T01:40:02.898458Z","shell.execute_reply.started":"2025-03-03T01:40:02.828329Z","shell.execute_reply":"2025-03-03T01:40:02.897507Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"id":"6af2f2d1","cell_type":"markdown","source":"## **6. Define Trainer and Train the Model**","metadata":{}},{"id":"bc07d267-22f4-4d59-9b5f-79aa509fa32a","cell_type":"code","source":"import torch\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:40:31.637198Z","iopub.execute_input":"2025-03-03T01:40:31.637580Z","iopub.status.idle":"2025-03-03T01:40:31.647441Z","shell.execute_reply.started":"2025-03-03T01:40:31.637551Z","shell.execute_reply":"2025-03-03T01:40:31.646626Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":9},{"id":"f42da630-230c-4f7d-a8c0-1d37823b3c3a","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7b67af81","cell_type":"code","source":"from transformers import TrainerCallback\n\nclass ConsoleLoggingCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None and \"loss\" in logs:\n            print(f\"Step {state.global_step} | Loss: {logs['loss']:.4f}\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    callbacks=[ConsoleLoggingCallback()]  # Attach callback\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:40:46.889153Z","iopub.execute_input":"2025-03-03T01:40:46.889457Z","iopub.status.idle":"2025-03-03T01:42:32.525811Z","shell.execute_reply.started":"2025-03-03T01:40:46.889435Z","shell.execute_reply":"2025-03-03T01:42:32.524844Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [189/189 01:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.387800</td>\n      <td>0.352611</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.288200</td>\n      <td>0.420528</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.191600</td>\n      <td>0.352753</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Step 10 | Loss: 0.6857\nStep 20 | Loss: 0.6542\nStep 30 | Loss: 0.5805\nStep 40 | Loss: 0.5238\nStep 50 | Loss: 0.4531\nStep 60 | Loss: 0.3878\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 70 | Loss: 0.3601\nStep 80 | Loss: 0.2949\nStep 90 | Loss: 0.2947\nStep 100 | Loss: 0.2978\nStep 110 | Loss: 0.2400\nStep 120 | Loss: 0.2882\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 130 | Loss: 0.2985\nStep 140 | Loss: 0.1852\nStep 150 | Loss: 0.1954\nStep 160 | Loss: 0.1644\nStep 170 | Loss: 0.2442\nStep 180 | Loss: 0.1916\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=189, training_loss=0.34375632692266395, metrics={'train_runtime': 105.2879, 'train_samples_per_second': 56.987, 'train_steps_per_second': 1.795, 'total_flos': 394666583040000.0, 'train_loss': 0.34375632692266395, 'epoch': 3.0})"},"metadata":{}}],"execution_count":10},{"id":"f8c221cd","cell_type":"markdown","source":"## **7. Debugging Issues During Training**","metadata":{}},{"id":"08542d95","cell_type":"markdown","source":"### **Possible Issues & Solutions**\n- **Overfitting**: Reduce epochs or increase dropout.\n- **Underfitting**: Increase training data or adjust learning rate.\n- **Long training time**: Use `distilbert` instead of `bert-base-uncased` for a smaller, faster model.\n\nTo experiment, try:\n```python\ntraining_args.num_train_epochs = 5  # Increase epochs if underfitting\ntraining_args.per_device_train_batch_size = 8  # Reduce batch size if memory issue\n```\n","metadata":{}},{"id":"3ea0fa27","cell_type":"markdown","source":"## **8. Evaluate Model Performance**","metadata":{}},{"id":"110835ca","cell_type":"code","source":"# Evaluate the model\neval_result = trainer.evaluate()\nprint(f\"Evaluation results: {eval_result}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:47:43.162401Z","iopub.execute_input":"2025-03-03T01:47:43.162857Z","iopub.status.idle":"2025-03-03T01:47:45.758221Z","shell.execute_reply.started":"2025-03-03T01:47:43.162827Z","shell.execute_reply":"2025-03-03T01:47:45.757445Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.3527529239654541, 'eval_runtime': 2.587, 'eval_samples_per_second': 193.276, 'eval_steps_per_second': 6.185, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":14},{"id":"38bd4e29-5c56-48af-8e10-3ef9842ba29e","cell_type":"code","source":"# Define compute metrics function\ndef compute_metrics(pred):\n    predictions = pred.predictions  # Extract predictions\n    labels = pred.label_ids  # Extract true labels\n    predictions = np.argmax(predictions, axis=1)  # Get predicted class\n\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n\n    return {\"accuracy\": acc, \"f1_score\": f1}\n\n# Make predictions\neval_predictions = trainer.predict(test_dataset)\n\n# Compute evaluation metrics\nmetrics = compute_metrics(eval_predictions)\n\nprint(f\"Final Evaluation Metrics: {metrics}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:47:53.281104Z","iopub.execute_input":"2025-03-03T01:47:53.281621Z","iopub.status.idle":"2025-03-03T01:47:55.881644Z","shell.execute_reply.started":"2025-03-03T01:47:53.281562Z","shell.execute_reply":"2025-03-03T01:47:55.880849Z"}},"outputs":[{"name":"stdout","text":"Final Evaluation Metrics: {'accuracy': 0.85, 'f1_score': 0.8520710059171597}\n","output_type":"stream"}],"execution_count":15},{"id":"cc6cf24c","cell_type":"markdown","source":"## **9. Apply Model to Real-World Task**","metadata":{}},{"id":"cef0b837","cell_type":"code","source":"# Example text inputs\ntexts = [\"This movie was fantastic! I loved every moment.\", \n         \"The film was terrible. I regret watching it.\"]\n\n# Tokenize inputs\ninputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n\n# Ensure inputs are moved to the correct device (if using GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninputs = {key: val.to(device) for key, val in inputs.items()}\nmodel.to(device)\n\n# Make predictions\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.argmax(outputs.logits, dim=-1).cpu()  # Move predictions back to CPU\n\n# Print results\nfor text, pred in zip(texts, predictions):\n    label = \"Positive\" if pred == 1 else \"Negative\"\n    print(f\"Review: {text} \\nPredicted Sentiment: {label}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T01:51:33.118233Z","iopub.execute_input":"2025-03-03T01:51:33.118790Z","iopub.status.idle":"2025-03-03T01:51:33.161787Z","shell.execute_reply.started":"2025-03-03T01:51:33.118748Z","shell.execute_reply":"2025-03-03T01:51:33.160969Z"}},"outputs":[{"name":"stdout","text":"Review: This movie was fantastic! I loved every moment. \nPredicted Sentiment: Positive\n\nReview: The film was terrible. I regret watching it. \nPredicted Sentiment: Negative\n\n","output_type":"stream"}],"execution_count":17},{"id":"330728ef","cell_type":"markdown","source":"## **10. Conclusion**","metadata":{}},{"id":"4bb7e41e","cell_type":"markdown","source":"In this notebook, we:\n- Fine-tuned `bert-base-uncased` on the IMDb dataset\n- Addressed common debugging issues\n- Evaluated the model using accuracy and F1-score\n- Applied the model to classify unseen text\n\n### **Next Steps:**\n- Try different datasets (e.g., SQuAD for question answering)\n- Experiment with hyperparameters for better accuracy\n- Deploy the model as an API for real-world applications\n\n**Thank you for exploring BERT with me! ðŸš€**\n","metadata":{}}]}