{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Artistic Abstract Images with GANs\n",
    "\n",
    "This notebook explores the use of Generative Adversarial Networks (GANs) to create unique and artistic abstract images.\n",
    "The project focuses on training a GAN using the [Abstract Art dataset](https://www.kaggle.com/datasets/greg115/abstract-art).\n",
    "\n",
    "## Objectives\n",
    "- Understand how GANs can generate visually diverse artistic outputs.\n",
    "- Train a GAN using a dataset of abstract art images.\n",
    "- Generate and evaluate the quality of artistic images produced by the model.\n",
    "\n",
    "---\n",
    "## Part 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Define dataset path\n",
    "DATA_PATH = '../input/abstract-art/'  # Adjust as needed for Kaggle Notebook\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize pixel values to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=DATA_PATH, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f'Dataset size: {len(dataset)} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building the GAN\n",
    "\n",
    "### Defining the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gan_model"
   },
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128*16*16),\n",
    "            nn.BatchNorm1d(128*16*16),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 16, 16)),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*16*16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the GAN\n",
    "\n",
    "Using an adversarial training loop to optimize both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_gan"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "num_epochs = 100\n",
    "lr = 0.0002\n",
    "\n",
    "# Initialize models and optimizers\n",
    "generator = Generator(latent_dim)\n",
    "discriminator = Discriminator()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_imgs, _) in enumerate(dataloader):\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(real_imgs.size(0), 1)\n",
    "        fake_labels = torch.zeros(real_imgs.size(0), 1)\n",
    "        real_loss = loss_fn(discriminator(real_imgs), real_labels)\n",
    "        z = torch.randn(real_imgs.size(0), latent_dim)\n",
    "        fake_imgs = generator(z)\n",
    "        fake_loss = loss_fn(discriminator(fake_imgs.detach()), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = loss_fn(discriminator(fake_imgs), real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - D Loss: {d_loss.item()} - G Loss: {g_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Abstract Art Images\n",
    "Visualizing the outputs from the trained generator.\n",
    "\n",
    "This section will:\n",
    "\n",
    "1.  Generate new images using the trained **Generator**.\n",
    "2.  Save and display the generated images.\n",
    "3.  Provide basic evaluation metrics and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generating and Evaluating Artistic Abstract Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and visualize abstract images from the trained GAN\n",
    "def generate_images(generator, latent_dim, num_images=16):\n",
    "    generator.eval()  # Set the generator to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Generate random noise vectors\n",
    "        z = torch.randn(num_images, latent_dim)\n",
    "        # Generate images\n",
    "        generated_imgs = generator(z).cpu()\n",
    "    \n",
    "    # Denormalize images (since we normalized them in range [-1, 1])\n",
    "    generated_imgs = (generated_imgs + 1) / 2  # Scale back to [0,1]\n",
    "    \n",
    "    # Plot the generated images\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(np.transpose(generated_imgs[i].numpy(), (1, 2, 0)))\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate and display new abstract art images\n",
    "generate_images(generator, latent_dim, num_images=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation: How Good Are the Generated Images?**\n",
    "\n",
    "Since evaluating abstract art is highly subjective, you can use:\n",
    "\n",
    "1.  **Visual Diversity**: Look at the variations in generated images.\n",
    "2.  **FID Score (Frechet Inception Distance)**: Measures similarity to real images.\n",
    "3.  **User Feedback**: Ask users if the generated images look like abstract art.\n",
    "\n",
    "For a basic FID Score evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Function to compute the FID Score\n",
    "def compute_fid(real_images, fake_images):\n",
    "    model = inception_v3(pretrained=True, transform_input=False)\n",
    "    model.fc = torch.nn.Identity()  # Remove the classification layer\n",
    "    model.eval()\n",
    "\n",
    "    # Convert images to InceptionV3 features\n",
    "    with torch.no_grad():\n",
    "        real_features = model(real_images).cpu().numpy()\n",
    "        fake_features = model(fake_images).cpu().numpy()\n",
    "\n",
    "    # Compute mean and covariance\n",
    "    mu_real, sigma_real = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_fake, sigma_fake = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "\n",
    "    # Compute FID score\n",
    "    diff = mu_real - mu_fake\n",
    "    covmean, _ = sqrtm(sigma_real.dot(sigma_fake), disp=False)\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real  # Convert to real numbers if complex\n",
    "\n",
    "    fid_score = np.sum(diff**2) + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid_score\n",
    "\n",
    "# Select real images from dataset and generated images for FID evaluation\n",
    "real_images, _ = next(iter(dataloader))  # Get a batch of real images\n",
    "real_images = real_images[:16]  # Take a subset\n",
    "fake_images = generator(torch.randn(16, latent_dim)).detach()\n",
    "\n",
    "# Compute FID Score\n",
    "fid_score = compute_fid(real_images, fake_images)\n",
    "print(f\"FID Score: {fid_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cognizant)",
   "language": "python",
   "name": "cognizant"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
